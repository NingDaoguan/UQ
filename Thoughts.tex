\documentclass[11pt]{article}
% RFP specifically says to use 11 point type and 1 inch margins
\usepackage{graphicx}
\usepackage{epsf,color}
\textwidth=6.5in\oddsidemargin=0in \evensidemargin=0in \topmargin
0pt \advance \topmargin by -\headheight \advance \topmargin by
-\headsep \textheight 9.0in

%\textwidth=6.5in\oddsidemargin=0in \evensidemargin=0in \topmargin
%0pt \advance \topmargin by -\headheight \advance \topmargin by
%-\headsep \textheight 8.9in

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage[compact]{titlesec}

%\usepackage[plain]{fullpage}
\usepackage{amsfonts}
%\usepackage{lastpage}
%\usepackage{fancyhdr}

\usepackage[version=3]{mhchem} 
% you can use this command to skip chunks of your document
% just put the command around the chunk like this
% \comment{ ...the chunk... }
\newcommand{\comment}[1]{}

%\newcommand{\MarginPar}[1]{\hspace{1sp}\marginpar{\tiny\sffamily\raggedright\hspace{1sp}#1}}
\setlength{\marginparwidth}{0.75in}
\newcommand{\MarginPar}[1]{\marginpar{%
\vskip-\baselineskip %raise the marginpar a bit
\raggedright\tiny\sffamily
\hrule\smallskip{\color{red}#1}\par\smallskip\hrule}}

\definecolor{drkgrn}{rgb}{0.043,0.341,0.2274}
\newcommand{\remrg}[1]{ {\it \color{drkgrn} \{#1 -RG\}}}

%\renewcommand{\baselinestretch}{1.05} % = 1.0 Single space; = 2.0 Double
\renewcommand{\baselinestretch}{1.0} % = 1.0 Single space; = 2.0 Double

%\renewcommand{\refname}{Literature Cited}
%------------------------

%\pagestyle{empty}  % No page numbers
%\textfloatsep 0mm
%\abovecaptionskip 1mm

\begin{document}

%\pagestyle{plain}
%\pagenumbering{roman}
\begin{center}
{\Large{\textbf{Project Thooughts}}}

\vskip\baselineskip
{\large{J. Bell, M. Day, J. Goodman, P. Graf, R. Grout, M. Morzfeld, G. Pau}}
\end{center}

Below is a summary of some discussion with Ray Grout and some ideas of Matti Morzfeld

\section{Attempted coherent project summary}

In a number of important problem areas, simulations require inputs describing the physical system
that contain significant uncertainty.  
Systems described by multicomponent reacting flows, for example, rely on models for the chemical kinetics and
transport properties.

Examples of this type include combustion, catalysis, thin-film
photovoltaics, geochemistry, systems biology, Li-ion and alternative
chemsitry batteries and wind energy; however,
the same basic paradigm is more broadly applicable.  The goal of this project is to develop a
mathematical framework for this class of problems that will allow us to (1) use available data to restrict
uncertainty in the description of the system, (2) estimate the impact of the improved characterization
on predictive capabitility and (3) identify which of the remaining uncertainties have the most impact
on the uncertainty of the predictions.

One important feature of the class of problems we want to consider is that there is a hierarchy
of experimental data that provides information about the underlying processes.  As one progresses
through this hierarchy, the problem becomes more complex and our ability to simulate it is reduced but
the data is closer to the target application.  An important aspect of the problem is the need to pass
information through the hierachy.  Another important feature of the problem class we plan to consider
is that the underlying dynamics, particularly further up the hierarchy, is chaotic.  Reactions in a turbulent
flow are a prototypical example of this phenomena.
Within the chaotic regime, we need to be able to extract metrics based on identification of characteristic
features rather than more traditional quantities of interest.

The class of problems we want to consider are non-linear and exhibit non-Gaussian statistics.
Consequently, we plan to focus our approach on Monte Carlo sampling techniques.
Monte Carlo sampling is well suited for parallel computer architectures because the computationally
expensive calculations (e.g. forward or adjoint/backward model runs) can be executed independently;
the necessary communication happens only via scalar quantities, so that very little data must be moved
across nodes.
The core problem with Monte Carlo techniques is the catastrophic scaling properties as a function of the
dimension of the probability space.  In particular, as the dimension of the space of parameters increases,
the number of simulations needed to obtain useful imformation explodes, making brute force Monte Carlo
infeasible for large-scale problems.
Central to our approach is the development of smart sampling technologies to dramatically improve
the performance of Monte Carlo approaches.  Examples of these new approaches is the implicit sampling
\MarginPar{Help needed here}
methodology developed at LBNL 
and affine sampling approaches developed at NYU, both of which show significant improvement
in Monte Carlo performance.  These sampling approaches typically involve solving non-convex optimization problems for which the global minimum can be more efficiently determined using stochastic optimization algorithms that utilize reduced order models~\cite{Regis:2007,Wild:2011uh}. 

To address the hierarchical nature of experimental data, we will adopt a Bayesian framework.  Starting
with an initial prior, as we move through the hierarchy of experiments and the associated models, we will view the posterior distribution at
one level of the hierarchy as defining the prior for the next level of complexity. \remrg{John said something the other day (with apologies for the misquote) that seems to fit here: ``We should view this as constraining the 
  input priors by the results of the simulation, and the constrained priors become the input to the next level of the hierarchy ''} \remrg{Each data point assimilated seems to require a comparison between a simulation and an experiment, but I think we should make clear that the `experiment' could be a simulation of
  greater realism/fidellity, especially in the first context of ROM mentioned below.}  By adopting this
approach, we can balance computation across the hierarchy, reducing the number of simulations needed
as the complexity of the simulation increases.   \remrg{and also ensure that the more expensive simulations
  are carefully chosen to (most?)  effectively constrain the input
  priors} 

To more efficiently determine the appropriate attractors to be used at each level, reduced order models will be used to quantify the sensitivities of the attractors prior to a full Monte Carlo simulation using the full forward model.  After posterior distribution of the inputs are determined, the uncertainties of the attractors can also be efficiently quantified using reduced order models~\cite{Marrel:2009cp,Pau:2013vq}.  We note that appropriate reduced order model need to be selected for the model at each level of the hierarchy, ranging from model reduction approaches for models with smooth solution manifolds to statistical approaches for complex models.  As noted above as well, one aspect of the problem class we plan to consider is that as the problem becomes more
complex, the underlying dynamics becomes chaotic.  In this case, the available experimental data
 samples the attractor.  We cannot hope to obtain a detailed match to that data; rather, we want
to extract features characteristic of the data (attractor) and compare those to analogous computational
features.  Our goal will be to use statistical reduced order models or machine learning algorithms to identify and classify
features. \remrg{Reduced order modelling appears in
  this framework in two contexts: (1) where we are assessing the
  accuracy of a given reduced order model, possibly intended as a
  surrogate for more costly models in an application context and (2)
  where the ROM is utilized as a surrogate for optimization. In
  practice, we will use ROM developed as domain application tools in
  the second context, while also providing a method for rigorously
  assessing their accuracy.}



%NEED SOMETHING ABOUT ROLE OF REDUCED ORDER MODELS.  I THINK ONE USE OF REDUCED ORDER
%MODELING IS TO GUIDE THE SMART SAMPLING APPROACHES IN MONTE CARLO.  ALSO SHOULD BE
%USEFUL IN SCALE BRIDGING BUT THAT IS MORE TENUOUS IN MY HEAD.

Examples:

\subsection*{Turbulence/Chemistry Interaction example:}

Inputs: Chemical kinetics and transport parameters with initial priors
 
Hierarchical data sources (experiments): 0d ignition, 1d laminar flames, shock-tube experiments
2D laminar flame DNS, 3D turbulent flames

Objectives:
\begin{itemize}
\item Use available data to reduce uncertainty in kinetics and transport
\item Estimate the impact of the improved characterization on predictive capabitility for turbulent flames
\item Identify which of the remaining uncertainties have the most impact on the uncertainty of the predictions
\end{itemize}
 
\subsection*{Point defect formation in thin-film photovoltaics:}
A key design need for high efficiency photovoltaic devices based on newer materials 
%, e.g. those based on CZTS absorbers, 
is the ability to control the carrier concentration to achieve n+ and
p+ doping resulting from point defects. The creation of various types
of defects (intrinsic ionic and electronic defects) can be described
by a set of kinetic reaction equations. For example, an abstract
metal-sulfide system could be described by a simplistic system of four
reactions expressing an oxidation reaction, the Schottky disorder and
creation of anion vacancies, where the cofficients in models for the
rates of these processes might come from either experiment or detailed
calculations of the energy barriers. 
%%
%%\begin{eqnarray*}
%%\ce{ \tfrac{1}{2} S2 -> S_s^x + V_m^{''} + 2 h^+ } \\
%%\ce{ NULL -> V_m^{''}+ V_s^{$\cdot \cdot$} } \\
%%\ce{ NULL -> e^' + h^{$\cdot$} } \\
%%\ce{ S_s^x -> \tfrac{1}{2} S2 + V_s^{$\cdot \cdot$} + 2 e^- } \\
%%\end{eqnarray*}
%%
%Near first-principles calculations such as electronic structure
%calculations based on density functional theory can provide enthalpy
%differences for the formation of these various point defects.
%
%Neglecting the entropic contribution to the Gibbs energy, this
%Equilibrium concentrations that can be used to coarsely estimate the
%variation of the material characteristics with the key parameters can
%be calculated based on enthalpy differences for each reaction that in
%turn can be estimated via DFT computations; for time dependent
%simulations, further constants are required for the kinetic rate
%expressions.
For the such a system, the ambient $S_2$ partial pressure is a key
parameter that when varied results in states with varied sensitivity
of the measurable quantities (electron/hole concentration) to the
parameter. In this simplistic case we can see by inspection that
reducing the uncertainty in the kinetic constants is best achieved by
experiments targeting this regime of high sensitivity, but in the more
general case a more rigorous framework such as proposed here is
beneficial. Further complicating matters time-dependent histories of
the external conditions during crystal growth can be expected to
access quasi-equilibrium states. For realistic systems, involving 10s
to 100s of such kinetic expressions, many such states with desirable
properties may exist.  As in the combustion example, a hierarchy of
simulations with varying physical realism can be performed, e.g.,
diffusion of the non-metal species ($S_2$ above) into the film can be
included and may be rate-limiting.  Direct measurement of the
concentration of arbitrary point defects is difficult and must be
inferred from more easily observed characteristics, leading a model
for the observation process $h(x)$ that may be quite complicated and
the result of a non-trivial simulation.


\subsection*{Missing issues / concerns}

\begin{enumerate}
\item Need to flesh out more examples
\item How should we address the extreme-scale computing aspects of this
\item Fix general flakiness and flesh out places where help is needed
\end{enumerate}

\newpage

\section*{Matti's raw notes}


\begin{itemize}
\item Essentially all problems are non-linear and exhibit non-Gaussian statistics and, therefore, Monte Carlo sampling will be at the core of modern and next generation UQ techniques
\item Monte Carlo sampling is well suited for parallel computer architectures because the computationally expensive calculations (e.g. forward or adjoint/backward model runs) can be executed independently; the necessary communication happens only via scalar quantities, so that very little data must be moved across nodes
\item Due to a catastrophic scaling of the number of Monte Carlo simulations required with the number of dimensions of the underlying probability space, brute force Monte Carlo is not feasible for large-scale problems, even on exascale machines
\item New sampling techniques, developed at LBNL have shown promise in UQ in engineering problems (O(10) dimensions, e.g. in robotics, tracking of satellites) and in small scale geophysical inverse problems  (and their UQ, O(500) dimensions). The geophysical inverse problems are large enough such that traditional Monte Carlo is hopeless (on current and future machines).
\item We have experience with new sampling techniques for UQ, inverse problems and target tracking
\item The "technology" is ready for the next step: explore applicability to large/extreme scale problems, in particular (i) use of extreme scale minimization techniques (derivative free minimization, adjoint calculations, limited memory quasi-Newton methods); (ii) minimizing communication between cores/nodes and data movement/storage during sampling and updating; (iii) integration with large (extreme) scale simulation codes; (iv) a study of the scaling of the algorithm with the number of cores
\item Result: first large scale UQ system with smart Monte Carlo sampling and without Gaussianity or linearity assumptions
\item UQ  capabilities: UQ without Gaussianity or linearity assumptions; integration of experimental data and mathematical model/numerical codes; accurate tracking of uncertainty (e.g. covariances of errors); forecasting under uncertainty.

\end{itemize}


\section*{Ray's raw notes}

Thematic elements:

Manifold discovery / slow attractors

Link input parameters to changes in slow attractor

Identify observables based on characteristics of slow attractor that are related to changes in input parameters, specifically those we wish to investigate

Design framework so that each experiment conducted narrows the distribution of the priors to provide a strong probability of finding a change in attractors or measurable difference in output. 

Searching for transformation between distribution of inputs and distribution of outputs. 

Quantify the utility of an experiment by the degree of reduction in uncertainty

Use guided search based on priors to restrict the search space

Turbulence/Chemistry Interaction example:


Inputs: Chemical kinetics parameters, geometry, initial conditions

Traditional outputs: net overall burning rate in turbulent flame at device conditions

Candidates for optimal observable: reaction rate of each of n species, creation/destruction rates, vector indicating participation in each possible reaction pathway, vector indicating species participation.
(These are based on physical insight -- is it necessary to presuppose the candidates for observables?)
 
Data sources (experiments) hierarchy: 0d ignition simulation, 1d laminar flame simulation, library of 1d laminar flame experiments at conditions that perturb expected chemistry, ODT based laminar flame solution, 2D turbulent flame DNS, 3D turbulent flame DNS, shock tube ignition delay time parametric study.

Objective: Determine which chemical kinetics parameters to study further; design experiments with appropriate observables to objectively compare different kinetic mechanisms; design experiments to assess the impact of changing fuels (and hence mechanisms).

Point defect formation application:
Inputs: Potential kinetic mechanism; estimated activation energies based on DFT calculations; process conditions:

Observables: Quasi-equilibrium concentration of electrons in conduction band. 

Candidates for optimal observable: Individual point defect concentrations. 

Experimental hierarchy: Deterministic calculation of 0D equilibrium state, parametric in environmental conditions; experimental film growth with parametric variation of surface pressure, T (shifts relative rate of reactions); film growth in light/dark (may enable/disable defect formation mechanism); deterministic computation coupled to 1D transport; stochastic sampling of defect formation rates (using kinetic rates as transition probabilities).

Outcome: Determine process conditions that increase the probability of a favorable point defect configuration. Find the metastable states /attractors; assess these for favorable properties; engineer process likely to produce them. 

Other application areas:
\begin{itemize}
\item Anything with a reaction/kinetic network with uncertain parameters
\item Quantifying ``usefulness'' of expensive experiments
\item Kinetics of biological biomass deconstruction
\item Cosmology
\item OPV degradation rate modeling
\item Groundwater decontamination modelling 
\end{itemize}

Requisite features of application area:
\begin{itemize}
\item Model that can relate input parameters to observables with a hierarchy of fidelity
\item Targeted for kinetic reaction networks (conceivably this could be relaxed)
\item Dynamic / chaotic system with an underlying attractive manifold 
\item Finite number of attractors that can be described by continuous characteristics
\end{itemize}


%\newpage

\bibliographystyle{plain}

\bibliography{george_rom.bib} 



\end{document}
