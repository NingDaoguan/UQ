\documentclass[11pt]{article}
% RFP specifically says to use 11 point type and 1 inch margins
\usepackage{graphicx}
\usepackage{epsf,color}
\textwidth=6.5in\oddsidemargin=0in \evensidemargin=0in \topmargin
0pt \advance \topmargin by -\headheight \advance \topmargin by
-\headsep \textheight 9.0in

%\textwidth=6.5in\oddsidemargin=0in \evensidemargin=0in \topmargin
%0pt \advance \topmargin by -\headheight \advance \topmargin by
%-\headsep \textheight 8.9in

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage[compact]{titlesec}
\usepackage{paralist}
\usepackage{url}

%\usepackage[plain]{fullpage}
\usepackage{amsfonts}
%\usepackage{lastpage}
%\usepackage{fancyhdr}

\usepackage[version=3]{mhchem} 
% you can use this command to skip chunks of your document
% just put the command around the chunk like this
% \comment{ ...the chunk... }
\newcommand{\comment}[1]{}

%\newcommand{\MarginPar}[1]{\hspace{1sp}\marginpar{\tiny\sffamily\raggedright\hspace{1sp}#1}}
\setlength{\marginparwidth}{0.75in}
\newcommand{\MarginPar}[1]{\marginpar{%
\vskip-\baselineskip %raise the marginpar a bit
\raggedright\tiny\sffamily
\hrule\smallskip{\color{red}#1}\par\smallskip\hrule}}

%\renewcommand{\baselinestretch}{1.05} % = 1.0 Single space; = 2.0 Double
\renewcommand{\baselinestretch}{1.0} % = 1.0 Single space; = 2.0 Double

%\renewcommand{\refname}{Literature Cited}
%------------------------

%\pagestyle{empty}  % No page numbers
%\textfloatsep 0mm
%\abovecaptionskip 1mm

\begin{document}

%\pagestyle{plain}
%\pagenumbering{roman}
\begin{center}
{\large{\textbf{A Hierarchical Approach to Uncertainty Quantification}}}
\end{center}

\subsection*{Background / Introduction}

Many important DOE applications rely on simulation to predict the behavior of complex physical systems.
However, these simulations depend on uncertain parameters describing the underlying physical system
that are obtained from a collection of
complex and noisy experiments whose reliability is hard to determine. 
Our ability to use extreme scale computing effectively will depend on our ability to reduce the
uncertainty in these data and assess the impact of that uncertainty on simulation results.

This proposal discusses the Bayesian approach to uncertainty quantification (UQ).
We propose to develop and assess the effectiveness of newly emerging techniques in the 
context of three important application areas: combustion modeling, 
battery simulation and design of high-efficiency photovoltaic devices.
However, the methodology will be broadly applicable to a
range of problems in chemical and materials science, systems biology, subsurface flow and climate 
to name a few.
Many of the techniques discussed below have been demonstrated individually in simple model problems.
We believe that important new mathematical and computational issues will arise in combining them 
and applying them to more complex nonlinear problems.

An important aspect of the problems identified above is that they are not probed using a 
single experiment.  
Instead, there is a hierarchy of experiments of increasing complexity that provide 
information about the relevant processes and parameter values.
We hope to combine data from increasingly complex experiments to obtain increasingly accurate
parameter estimates.
The challenge is to assess the remaining uncertainties accurately.
We believe that Bayesian UQ provides the most general, flexible, and rigorous way to do this.

We need an approach that allows us to pass information through the hierarchy in a way that 
effectively uses data from all levels to improve overall predictive properties.
Furthermore, even complex models (e.g., the systems of equations below modeling Lithium-ion batteries)
cannot exactly capture the rich physics exactly.
We must develop estimation methods that are robust to model errors as well as noisy observations
using metrics based on identification of characteristic features that are more general than
traditional quantities of interest.
This robustness will allow us to develop reduced order models (ROMs), and also to use them 
within Monte Carlo (MC) samplers for efficiency. 

An implementation of the Bayesian UQ approach via MC sampling
is attractive because it respects the strong nonlinearities in the target applications
and avoids the need for simplifying assumptions/approximations required by some current UQ techniques.
Moreover, some MC sampling methods are well suited for massively parallel computer
architectures because the computationally expensive calculations,
e.g. forward or adjoint model runs, can be executed independently and
the communication between cores for each independent run is small.
Using MC sampling as the computational backbone will lead to a numerically sound
implementation of a rigorous UQ theory that is well suited for extreme
scale machines, making UQ possible for realistic and relevant applications.

Bayesian UQ replaces traditional parameter estimation and error bars with a single {\em posterior}
parameter distribution $p(\theta|y)$. 
Here, $\theta$ represents the uncertain parameter values and $y$ is the experimental data.
%see (\ref{eq:post}) for more detail.
For example, if the data tightly constrain the parameters, then $p(\theta|y)$ is narrowly 
distributed about some most maximum likelihood value $\theta_*$.
Rather than reporting, for example, the mean and covariance of $\theta \sim p(\cdot|y)$,
modern Bayesian UQ represents $p(\cdot|y)$ by giving a collection of random samples $\theta_k$.
These samples can be used to directly assess the parameter uncertainties.
They also can be used to assess the reliability of predictive simulations.
A simple, but computationally inefficient,  way to do this would be to repeat the simulations with different $\theta_k$.
A less computationally intensive approach would be to compute the sensitivity of the predictive 
simulation to parameter perturbations using the adjoint method, and then plug in $\Delta \theta_k$
directly or use a suitable reduced order model.

The posterior distribution $p(\theta|y)$ is extremely hard to calculate, and to sample, in the 
application areas listed above using data from experiments as complex
as is typical in these applications and outlined below.
For one thing, evaluating $p$ involves a forward simulation of a physical model.
%of the form (\ref{eq:ModelEquation}) or (\ref{eq:ModelEquationSt}).
\MarginPar{JBB:  Don't like forware references}
In our applications, these can be 3D simulations with complex, potentialy noisy physics and noisy observations.
When the dynamics are noisy, the quantities to be sampled include not just parameter values $\theta$,
but an unobserved state $x$.
\MarginPar{JG: Maybe it's the least of 3 evils, the other two being vagueness and verbal description
of formulas, or stopping to define notation in a technical way in the intro.
I'm basically only saying that $p$ is a probability density.}
%This can be a 3D field of velocities and concentrations (e.g., in the combustion system).
In such a case, the posterior joint distribution of $x$ and $\theta$ is described by $p(x,\theta|y)$.

A key issue is that many current MC sampling techniques are not robustly effective for complex, 
high-dimensional problems. 
There is a weak analogy to the problem of solving large systems of linear or nonlinear equations.
For equation solving there are generic methods such as Gaussian elimination, Gauss Seidel iteration,
Newton's method, etc.
But these generic methods are unable to solve extreme scale problems that arise from simulating
complex physics in 3D.
Such problems require specialized multi-scale algorithms, such as multigrid, that take into account the
physical nature of slowly relaxing modes.
What this analogy fails to capture is an extra difficulty in sampling: these problems do not come with 
something like {\em residuals} that can be evaluated to confirm convergence of the algorithms.
There is no way to test a-posteriori that a sampler has converged to the correct distribution.
The correctness must be built in to the sampler.

We plan to consider two kinds of MC samplers, Markov chain Monte Carlo (MCMC) and particle filtering.
Standard MCMC samplers can become very slow when the distribution being
sampled is ill conditioned.
Ill conditioning in problems of moderate dimension can come from approximate degeneracy: very
different combinations of parameters fit the data nearly as well.
Several approaches are emerging to address this, including multi-stage adaptive samplers and
affine invariant ensemble samplers.
High dimensional problems arise in our problems when we must sample a state, $x$, as well as a
parameter set $\theta$.
Hamiltonian samplers have proven better than direct local Metropolis or heat bath methods \cite{Hanson2001}, but
they still can be slow.
More sophisticated truly multi-scale samplers include Swendsen Wang \cite{SwendsonWang1987}
and multigrid methods \cite{Goodman1989}.
Unfortunately, none of these methods at present is as general, say, as multigrid for
solving discretizations of PDEs.
Some promising developments include ``chainless'' approximate methods \cite{Chorin:2008}, and
{\em parallel marginalization} \cite{Weare2007}. \MarginPar{RG: why italics?}
We will look for multiscale sampling methods that are effective for problems that come from PDEs
with noise under partial observation.

Particle filter approaches seem promising particularly for time dependent simulation and observation,
and when we want to take into account increasingly complex experiments.
Standard particle filters first obtain samples of the prior (see (\ref{eq:post}))
and then take the posterior into account by attaching weights to each sample based on its 
likelihood \cite{Doucet2001,GordonSIR}.
The weighted particles form an empirical estimate of the posterior.
Difficulties arise as the prior and posterior become mutually
singular, i.e. when high-probability 
\MarginPar{RG: This could also
 indicate model is inconsistent with data given prior.}
events with respect to the prior have low probability with respect to the posterior.
However, this is the interesting case, which corresponds to the situation where the data severely 
constrain the posterior beyond what was assumed in the prior.
When this happens many of the samples have such a low weight so that the empirical estimate 
they give little information about the posterior.
This was rigorously shown to happen for linear problems.
Futher, the number of particles required can grow exponentially with the number
of parameters being estimated \cite{Bickel,BickelBootstrap,Bickel2,Snyder,Weare2012,Weare2009},
making this approach infeasible for 
our target applications.
\MarginPar{JG says (Not for inclusion in the proposal): a nonlinear version of the Bickel example shows just
as easily that implicit sampling can be exponentially bad in the number of parameters.  
Just have $n$ independent non-gaussian parameters.}
A number of methods have been invented to ameliorate this problem, most of which amount to finding a proposal density that  generates samples that are more compatible with the data \cite{Doucet,OptimalImportanceFunction,liuchen1995,Brad}.
For example, a nudging technique was proposed in meteorology, however the method is not theoretically sound, requires a lot of ad hoc tuning  and is not easily applicable to parameter estimation \cite{vanLeeuwen}.

A more general approach constructs a map that transforms the prior measure into the posterior measure
%, i.e. to construct a map that transforms prior samples into posterior samples 
\cite{Moselhy2013}.  If such a map can be constructed, efficient posterior samples are easy to generate.
However, the construction of the map is theoretically and computationally expensive and, therefore, relies on a number of approximations (e.g. polynomial representations of the maps) that are not efficient.
We expect that we will need to compute a map at each level of the hierarchy and, thus, expect that this approach is computationally too expensive for our target applications. Moreover, the errors of the approximations involved in finding the map are, at this time, not well understood. We plan to base our particle filtering methods on the implicit sampling methodology invented at LBNL \cite{chorintupnas,chorin2010,Morzfeld2011,Morzfeld2012,Atkins2013} that avoids some of the pitfalls of standard sampling by concentrating the computational power on localizing the regions of high probability in the parameter space (see below for details of this method).

The posterior samples obtained from sampling (either MCMC or implicit sampling) can be used to construct reduced order models (ROMs), by which we mean inexpensive (to evaluate) surrogates for expensive computer simulations that preserve the input-output relations in the numerical models.
Here, we will use the term ROM in an encompassing sense which can refer to model reduction approaches, response surface approaches and machine learning techniques. For example, in model reduction approaches, e.g. the proper orthogonal decomposition method~\cite{Cardoso:2009jn, Lieberman:2010dw, Willcox:2002uk}, reduced basis method~\cite{Prudhomme:2002ug,Quarteroni:2011jm} and trajectory piecewise linearization procedure~\cite{Cardoso:2010, Rewienski:2003tr}, one directly approximates the PDEs using a parametrically-induced approximation space.
However, these techniques are difficult to apply to complex, multiscale models.

%Response surface and machine learning techniques
%such as the Gaussian process regression (GPR)~\cite{Rasmussen:2006vz},
%artificial neural network (ANN)~\cite{Cybenko:1992vo}
%and polynomial chaos expansion (PCE)~\cite{Oladyshkin:2012ur} are simpler to use,
%but possess their own severe limitations. 
\MarginPar{JG objects severely and loudly to saying anything at all about PCE except that it 
doesn't work.
Not only is that his view, but he has turned in several high profile negative DOE reviews saying as 
much.  
The program officers know his views and would raise their eyebrows. JBB:  I just deleted the whole bloody paragraph. Said
a bunch of stuff doesn't work and didn't say pce doesn't work loudly enough}
%PCE, in particular has been applied successfully to a limited class of flow problems (e.g. \cite{GhanemDham1998, LeMaitre2001}). 
%In PCE methods, the probability space of the parameters is decomposed into a finite set of univariate polynomials with local or global support, and coupled together through tensor products. 
%These bases functions are propagated through the model, either ``intrusively'' or ``unintrusively''.  Intrusive schemes utilize, e.g., a Galerkin projection to reformulate the governing equations into a system governing the PC mode strengths directly; unintrusive methods numerically evaluate the modes through deterministic or random sampling of the original model/code. 
%However, strong nonlinearity and complex unsteady dynamics present significant challenges for this approach \cite{Mathelin2004}. 
%The difficulties arise primarily from the high-order expansions necessary to capture the modes with sufficient accuracy.
%In dynamic simulations, PCE usually requires a rapidly increasing expansion order to maintain a constant accuracy in time. 
%Specialized applications exist for oscillatory systems (e.g. vortex-shedding \cite{LucorKarniadakis2004,Xiu2002}),
%including phase-locked wavelet-based approaches \cite{WitteveenBijl2008}, but the general time-dependent problems of
%interest here remain computationally intractable for PCE. 
%GPR and ANN methods offer no relief, since much of the functional relationships between the outputs and parameters of
%interest encoded within the numerical models are lost, and must be indirectly captured through a prohibitively large set of training data.  

%There are several areas in which ROMs can play a role in a MC algorithm. 
A research question we will address is how to combine ROMs with MC algorithms as we move through the hierarchy of experiments.
For example, coarse resolution (thus computationally cheap) models have been used to generate  samples
during the importance sampling stage~\cite{Higdon:2002vx,Christen:2005wp,Efendiev:2007uw,Bal:2013tp} or
coupled with fine resolution models through the metropolis coupled chain~\cite{Higdon:2002vx}.
To further improve the match between the coarse and fine posterior distributions, Bal et. al.~\cite{Bal:2013tp} used
a zero-mean Gaussian model to approximate the discretization error~\cite{Kaipio:2007ux}.
However, since our simpler models may involve significant simplifications of the physics or reduction of spatial dimensions,
ROMs may be needed to more accurately model the structural variations of the difference between the models. 
Gaussian process regression is a potentially attractive form of ROM to capturing differences in resolution of models.  
ROMs can also be used directly as an efficient surrogate in forward UQ~\cite{Challenor:2012uv, Ratto:2012tf} for example in predicting catastrophic events and obtaining global sensitivity analyses indices that converges slowly with number of samples.   In this proposal, we will look into how ROMs can further improve the efficiency of the importance sampling approach that we will use.  In addition, we will determine how ROM can be used within our hierarchical framework to efficiently obtain an accurate predictions to pin the forward UQ.\MarginPar{RG: huh?}


\subsection*{Proposed Research}

We begin by outlining three target applications.
These applications raise a series of unique challenges that do not arise in smaller or linear
model problems.
They involve sparse data, multi-scale and multi-physics, and severe ill conditioning.

\subsubsection*{Combustion, turbulent lean flames}
\MarginPar{JG edited (i.e. made stuff up) to make this section more specific in order to
make it more like the other two, which are pretty specific.}

Flames are lean or rich depending on the amount of fuel.
Lean flames are preferred in some industrial applications because they generate less heat, and 
therefore less undesired byproducts.
They also are more likely to be turbulent and harder to simulate.
Unfortunately, reaction parameters that are measured in simple rich flame experiments may be
different in unsteady lean flames.
It is therefore important to infer some lean flame parameters directly from measurements of flames with
complex spatial and temporal structure.

We seek to utilize data from increasingly complex experiments ranging from 0d ignition, 
1d laminar flames, shock-tube experiments, and
a variety of 2D laminar flames to 3D turbulent flames.
Each experiment can have a number of distinct sets of data that focus on different
aspects of the flame and emphasize different combinations of parameter ranges.
Some experiments measure velocities using particle image velocimetry. Other experiments
measure composition at a given location using Raman spectroscopy. Others use planar laser induced
fluorescence to indirectly measure a particular species in a cross section of the flow.
One could also augment the data with non-reacting flow experiments that focus on transport properties.
Each of these experiments reveals information about the system (some are used by kineticists already). 
We want to use experimental data across the entire hierarchy of experiments to estimate a full 
parameter set more accurately. 
The Bayesian UQ approach will quantify the remaining uncertainties.

As we move up the hierarchy to more complex experiments, the datasets get larger, but the data 
become sparse in that they measure a small fraction of the concentration or velocity fields 
of a given turbulent flow.
The data do not give a complete picture of the flame at a given time. 
A reasonable cartoon for a turbulent flame experiment is that it corresponds to chaotic attractor 
of a dynamical system.
In this case, the dynamics are approximated by the multicomponent reacting Navier Stokes equations 
with Arrhenius kinetics.
Simulation and MC sampling will produce sample velocity, concentration fields, and parameter sets
consistent with these large but incomplete datasets.


\subsubsection*{Thin-film photovoltaic materials}
Our second  application 
is the fabrication of thin-film photovoltaic
materials. 
Thin film photovoltaic (PV) devices have attracted research attention
as a way to dramatically reduce the cost of solar panels. These
materials compete with traditional crystalline silicon in efficiency but use far less
material \cite{JiangY13}. 
%Leading thin film technologies include absorbers based on
%cadmium telluride (CdTe), Copper-Indium-Gallium-Selenide/Sulfide (CIGS) and 
One candidate material is $Cu_2ZnSnS_4$ (CZTS) \cite{JiangY13}, which
% In contrast
%to the exotic elements in CdTe and CIGS, CZTS is
comprised of relatively abundant and benign constituents but is not yet mature relative to more established thin film technlogies. 
%. However,
%despite considerable attention CZTS development is still not mature
%relative to CdTe technology. The performance is sensitive to the
%fabrication process: 
Leading  CZTS  performance has been obtained by a variety of processes including depositing the film on a substrate via
solution-processing (11\% conversion efficiency) \cite{Todorov13},
co-evaporation (9.15\% efficiency \cite{Repins12})) and vacuum
process (8.4\% efficiency, \cite{Shin11}). Despite theoretical
efficiencies exceeding 30\%,  optimum processing conditions
that realize the potential remain elusive.
The challenge here is to 
%refine estimates of the model parameters
%sufficiently to improve predictions of the process
%from measurements of bulk
predict performance properties that result from uncertain and time-varying process
conditions.

Unlike crystalline silicon that is explicitly doped
% with phosphorous
%or boron 
to create  semiconductors, CZTS is
`self-doped' through point defects.
% in the kesterite crystal. Creation
These defects (vacancies, antisite and interstitial), particularly
the $Cu_{Zn}$ antisite defect for p-type doping in CZTS, provide intrinsic 
%result in an
%increase in carrier concentration and 
semiconductor behavior
\cite{JiangY13}. The overall defect formation/destruction, movement
through the crystal structure and along grain boundaries can be
described by a set of coupled stochastic partial differential
equations that treat the defects as continuum
quantities:
\begin{equation}
  \label{eq:pdpde}
  \frac{\partial \phi_j}{\partial t}  = R_j + \nabla \cdot  D_j \nabla \phi_j  + T_j,
\end{equation}
where $\phi_j$ represents the number density of $j^\mathrm{th}$ type
of defect or election/hole (carrier) concentration. 

The source term $R_j$ is local rate of change due to pseudo-reactions
that govern the creation / destruction and interaction of point
defects, $D_j$ is a defect-dependent mobility, and $T_j$ represents
a stochastic model for transport at grain boundaries.
A simple example of the `reactions' for a metal-X system  can
be described by a simplistic system of four reactions:
\begin{eqnarray}
\label{eq:pdrxnsstart}
\ce{ \tfrac{1}{2} X_2 -> X_x + V_m^{''} + 2 h^+ } \\
\ce{ NULL -> V_m^{''}+ V_x^{\cdot \cdot} } \\
\ce{ NULL -> e^ '+ h^{\cdot} } \\
\ce{ X_x -> \tfrac{1}{2} X2 + V_x^{\cdot \cdot} + 2 e^- }.
\label{eq:pdrxnsend}
\end{eqnarray}
Here, $\ce{X_2}$ is a non-metal gas  that is interacting
with the crystal at the interface. The four reactions describe:  the
addition of an $\ce{X}$ atom to the lattice producing cation vacancies
($V_m^{''}$),  creation and elimination of vacancies
(``Schottky defects''),  creation/elimination of electron/hole pairs
(``electronic defects'') and an X ion in the lattice
dissociating into an X atom that leaves the lattice and two elections,
leaving behind an anion vacancy ($V_x^{\cdot \cdot}$) \cite{Tilley}. 
%The equilibrium concentration
%of these defects are non-zero because the free energy of the crystal
%is lowered in the presence of such defects; 
In a more complicated
system such as CZTS many more defects are possible. 
%Electronic
%structure calculations \cite{Walsh12} provide a wealth of information
%indicating the important physical processes and configuration effects,
This formulation is useful because electronic structure calculations are unable to capture the long time-horizon dynamics necessary to
couple the processing conditions to the concentrations. The rates of
the reactions in (\ref{eq:pdrxnsstart}--\ref{eq:pdrxnsend}) are given by Arrhenius forms
($\omega_\alpha = k_\alpha e^{-(E_a)_\alpha/(RT)}$) and assembled
into the source terms $R_j$ based on a linear combination given by the
coefficients for the $j^\mathrm{th}$ species in the
$\alpha^\mathrm{th}$ reaction.

Transport of the point defects within the crystal occurs in two
regimes. Solid state diffusion is frequently approximated using a
Fickian treatment $\nabla\cdot D_j \nabla \phi_j$;
however, for point defects this is insufficient to account for their
migration along grain boundaries, where the
transport mechanism becomes delocalized. The higher
probability of the defect moving along the grain boundary can be
captured by a stochastic model; $T_j$ represents stochastic transport
that is active only in the vicinity of a grain boundary (see  Kolluri and
Demkowicz \cite{Kolluri12} and Koptelov et al. \cite{Koptelov84} for  potential treatments). 
%looked into transport of delocalized
%defects at interfacial grain boundaries for CuNb and found that it can
%be modelled as a complication on localized defect migration and
%successfully treated based on transition state theory with a
%temperature dependent pre-factor. \MarginPar{(matti) can you check the last sentence? I am a bit %confused here.}

\MarginPar{JBB:  Unable to parse this sentence. RG: is this better?}
Because the point defect transport and chemistry is coupled to grain structure, a model
for grain nucleation and growth is also necessary.
The grain growth model is important in its own right because the
presence of secondary phases on device performance are largely detrimental to device
performance \cite{Flammersberger} and annealing conditions is an important processing step.
%Grain structure has been treated by numerous methods that attempt to capture the driving force for grain growth
%( to first order, the reduction in grain energy captured by the curvature of the interface and interactions between the grain)
%including interface tracking and continuum descriptions. The latter, where grain boundaries are identified as misaligned orientations
%between adjacent points on a (typically cartesian) has been formulated as a deterministic \cite{FanGC97} or stochastic
%\cite{Rollett04} problems and is conveniently compatible with the continuum representation of point defect kinetics. 
%Both formulations involve numerous phenomenological parameters with associated uncertainties. 
A number of approaches have been used to model grain boundary
dynamics.  Here we focus on a stochastic approach 
used by Rollett \cite{Rollett04} that is compatible with the continuum representation of point defect kinetics. 
%In the deterministic treatment by Fan et al., a set of ODEs is written for a finite number $p$ of continuous orientation field variables at each grid point $\eta_1, \eta_2(x), \dots, \eta_p(x)$. The total free energy is then written in terms of the local free energy density $e_0$ and gradient energy coefficients $\kappa_i$:
%\MarginPar{JBB:  may need to shorten this.  Can we credibly attack this complexity}
%\begin{equation}
%  \label{eq:detgrainE}
%  E = \int_V e_0\left( \eta_1, \eta_2, \dots, \eta_p \right) + \sum_i^p \frac{\kappa_i}{2} \left( \nabla \eta_i \right)^2 dx.
%\end{equation}
%Fan et al. use a function form for the free energy density involving phenomenological parameters $\alpha, \beta, \gamma$:
%
%\begin{equation}
%  \label{eq:detgraine0}
%  e_0 = \sum_i^p \left( - \frac{\alpha}{2} \eta_i^2 + \frac{\beta}{4}\eta_i^4\right) + \gamma\sum_i^p \sum_{j\ne i}^p \eta_i^2\eta_j^2,
%\end{equation}
%and used the Ginzburg-Landau equations involving kinetic coefficients describing grain boundary mobility $M_i$ to evolve the orientation field variables:
%\MarginPar{JBB what is $L_i$}
%\begin{equation}
%  \label{eq:detgrainGL}
%  \frac{d \eta_i}{dt} = -L_i \frac{\delta E}{\delta \eta_i}= -L_i \left( \frac{\partial e_0}{\partial \eta_i} - \kappa_i \nabla^2\eta_i \right)
%\end{equation}
Each possible orientation is enumerated {\it{a priori}} into an
orientation index. Each gridpoint 
is assigned an orientation index $S$, with grains corresponding to contiguous regions with constant S.
The total system energy is defined as:
\begin{equation}
  \label{eq:mcgrainE}
  E = \sum_j^N \sum_i^n J(S_i, S_j) (1-\delta_{S_iS_j})
\end{equation}
where the inner sum is taken over the $n$ first and second nearest neighbors of the $i^\mathrm{th}$ element and $J(S_i,S_j)$ is the energy of a unit of boundary between elements $i$ and $j$ with orientation indicies $S_i$ and $S_j$. The transition probability for a reorientation with energy difference $\Delta E$ and mobility $M$ beween indicies $S_i$ and $S_j$ is:
\begin{equation}
  \label{eq:mcgrainP}
  P( S_i, S_j, \Delta E, T) = \left\{ \begin{aligned}
      & \frac{J(S_i,S_j)}{J_\mathrm{max}} \frac{ M(S_i, S_j) }{M_\mathrm{max}}  \quad \Delta E \le 0 \\
      & \frac{J(S_i,S_j)}{J_\mathrm{max}} \frac{ M(S_i, S_j) }{M_\mathrm{max}} \exp \left( \frac{-\Delta E}{TJ(S_i,S_j)}\right)  \quad \Delta E > 0 \\
      \end{aligned}
      \right. 
\end{equation}
In the above, the energy between orientation indicies $J$ and mobility
$M$ are experimentally derived parameters (that are only approximately
knwon) for each
%
orientation pair that reflect preferential growth along a particular misalignment direction.
The state of the crystal is updated sequentially by choosing random locations and new orientations that
are accepted based on the given probability. 
\MarginPar{JG: This seems to be an atomistic description, not a continuum?  
That's OK for MCMC sampling, but not (I don't think) for implicit sampling. RG: no, continuum - the `lattice site' term was wrong, should be `gridpoint'}


%\remrg{The stochastic formulation appears to be calling out for MCMC. This aspect of the thin film photovoltaic application readily generalizes and is applicable to a broader range of materials problems, where the objective is to predict grain size after an annealing process to design materials with specific mechanical as well as electrical properties.}


  Solution of the combined system yields the $e^-$ and
  $h^+$ carrier concentrations that reflect device
  performance. Point defects are generally not directly measurable;
  instead, they are inferred from measurements of carrier
  concentration (via resistance, conductivity). As with the combustion
  example, a hierarchy of experiments is available and typically used
  to focus on the effect of different parameters.
%The temperature
%  sensitivity of such measurements can be used in combination with
%  either a selection of conditions where a particular type of defect
%  governs the response (not general) or in combination with the
%  solution of the full system as outlined above. A desirable workflow
%  for UQ in this application is to use the experiments available to narrow
%  the uncertainty in the kinetic and transport parameters as far as
%  possible, and then search for processing pathway that provides
%  optimum result given remaining uncertainty in parameters.

%  Overall, this application has several aspects that differentiate it
%  from the others insofar as it has an inherently stochastic component
%  (in the transport at the grain boundaries and also, in the more
%  sophisticated models, in the reactions describing the surface
%  kinetics), it exhibits very large uncertainty in relatively few
%  parameters (the bounds on the kinetic constants are very wide) and,
%  as it is somewhat less well studied than the combustion system,
%  there is a significant opportunity for model insufficiency. The
%  final feature provides an opportunity to test the robustness of
%  mechanisms to detect when a model is inconsistent with data.

%%%%%%%%%%% batteries
\subsubsection*{Energy storage}
Our third application area concerns lithium-ion batteries (LIBs).
A recent DOE Basic Energy Sciences (BES) workshop report states: 
``Revolutionary breakthroughs in Electrical Energy Storage have been singled out as perhaps the most crucial need
for this nation's secure energy future''  (\cite{ees_rpt}, p. xii).
The promise of electric-drive vehicles (EDVs) can only be met if batteries having both high energy density and increased safety 
and reliability are developed.  Current designs have multiple limitations,
including exploding laptops and expensive electric vehicles that burst into 
flame and/or can only travel 100 miles before requiring a full night to recharge, and are thus inadequate for widespread adoption.  
LIBs present a truly multiscale system, with important macroscopic behavior
(e.g., catastrophic failure such as fire) determined by intricate microscale
interactions (e.g., as a result of dendritic growth on lithium particles in
one location deep inside the battery pack). 
%It is extremely difficult, where
%possible at all, to examine the mechanisms involved---especially, to
%elucidate the microscopic origin of macroscopic phenomenon---experimentally.
The highly structured, heterogeneous nature of battery systems
requires a multiscale model; they cannot be adequately modeled with a purely device-scale system. 
In theory one can enumerate at least 6 levels of detail that are tied together in some way: 
atomistic (quantum), atomistic (MD), particles, electrode, cell, pack. Here we
focus on the electrode and cell scales.
%
%\noindent{Particle scale}
%%\begin{itemize}\itemsep -0.0em
%\begin{compactitem}
%%\item
%%atomistic, quantum:  (Schr\"{o}dinger) $ H \Psi = \epsilon \Psi$, where $H$ is the Hamiltonian operator, and 
%%$\Psi$, $\epsilon$ are the wave functions and eigenvalues.
%%\item
%%atomistic, molecular dynamics: (Newton's law) $\frac{1}{m}\frac{d^2x}{dt^2} = -\frac{dU}{dt}$, where
%%$x$, $m$ are atomic positions and masses, and $U$ is the potential energy.
%\item
%Li concentration $c_s$ (diffusion): $\frac{\partial c_s}{dt} = \nabla \cdot (D_s \nabla c_s)$, where
%$D_s$ is the diffusivity of Li in Li-hosting particles.
%\item
%particle-electrolyte interfacial kinetics, e.g., Butler-Volmer: $I = \exp(\alpha_a \eta(I)) - \exp(\alpha_c \eta(I))$.  The overpotential $\eta(I)$ is a function of Li current $I$ across the interface, and $\alpha_a$, $\alpha_c$
%are anode and cathode charge transfer coefficients.  This equation can be derived under certain assumptions from more general chemical kinetic equations.
%\end{compactitem}
\MarginPar{multiscale discussion now only talks about 2 scales . . . probably ambitious enough}

\noindent{Electrode scale}
\begin{compactitem}
\item
Li concentration $c_e$ (convection-reaction-diffusion): $\frac{\partial c_e}{dt} = \nabla \cdot (D_e \nabla c_e) + \nabla (v c_e) + R_{c_e}$, where $D_e$ is Li diffusivity in electrolyte, $v$ is the 
convective velocity (a function of electric potential $\phi$), and $R_{c_e}$ is the reaction rate (solution of, e.g., Butler
Volmer). 
\item
electric potential $\phi$ (reaction-diffusion): $\frac{\partial \phi}{dt} = \nabla \cdot (D_{\phi} \nabla \phi)  + R_{\phi}$, where $D_{\phi}$ is charge diffusivity in electrolyte, $R_{\phi}$ is a volume averaged current density entering electrolyte.
\item
stress field, $\sigma$, e.g., Hooke's law for chemically active solid: $\sigma_{ij} = C_{ijkl}^{Li} [\epsilon_{kl} - \beta_{kl}(c_e - c_{e_0})] $, where $c_{e_0}$ is equilibrium concentration, and $C$, $\epsilon$, $\beta$ are stress, strain, and Vegard coefficients.
\item
equilibrium stress, $\sigma$ (constitutive equation): $\nabla \cdot \sigma = 0$ 
\end{compactitem}

\noindent{Cell scale}
\begin{compactitem}
\item
electric potential $\Phi$ (reaction-diffusion): $\frac{\partial \Phi}{dt} = \nabla \cdot (D_{\Phi} \nabla \Phi) + R_{\Phi}$,
where $D_{\Phi}$ is charge diffusivity in cell/pack, $R_{\Phi}$ represents charge injection from electrode scale. 
\item
temperature $T$ (thermal transport): $\frac{\partial T}{dt} = \nabla \cdot (D_T \nabla \Phi) + R_T$, where $R_T$ is heat flux from electrode scale, $D_T$ is heat diffusivity.
\end{compactitem}

\MarginPar{JBB:  reduced for space. . only 2 scale so only 3 order of magnitude}
%given systems at particle, given system at electrode and given system as cell}
%Uncertainty quantification in this context represents an
%extreme scale challenge:
%each scale alone is a 3D simulation in a heterogeneous domain;
%the difference in scale between the first and last set of equations represents approximately 3-4 orders of magnitude that must be
%%resolved and
%coupled in some way;
%many such hierarchically coupled runs need to be executed to perform a UQ study.
  
%``Simulation of lithium-ion battery models requires simultaneous evaluation of concentration and potential fields, %in both solid as well as liquid phases. In addition, the porous nature of the battery electrodes leads to highly %nonlinear and heterogeneous electrochemical reaction kinetics." \cite{Subramanian:2009}

Examples of battery models include \cite{Less:2012}, in which a fully resolved 3D electrode is simulated using a combination
of convection-diffusion and Butler-Volmer equations. This model uses approximately 25 parameters derived by the authors from experiment.
Another example is \cite{Garcia2005}, which simulates a 3D electrode, including the constitutive relations that are first steps toward
incorporating stress/strain in the models. This model contains approximately 50 parameters.  Another interesting case is \cite{Kim-etal:2011}, where models at three scales (particle, electrode, cell) are coupled via boundary conditions and forcing terms. This model also contains roughly 50 parameters.

The current paradigm for virtually all battery simulations (including those above)
is to treat parameters as fixed valued derived from single-scale experiments.
However, this is problematic: first, we do not propagate uncertainty in 
these parameters through the model; second, we miss the chance to use information \emph{across scales} to optimally understand
the system.  Our proposed work specifically addresses this shortcoming in the current paradigm by offering the means to 
combine modeling and data \emph{across a hierarchy of experiments}
to reduce uncertainty in the underlying physics parameters and improve predictive capability.

Hierarchies of experiments exist that address virtually all aspects 
of the heterogeneous battery system.  For example, some representative properties and measurements across scales include:\\
\textbf{diffusivities}: (microscale) single-crystal measurements, molecular diffusivity  \cite{Chung:2011}; (mesoscale) in-situ/single particle measurements \cite{Cui:2012};  (macroscale) galvanostatic intermittent titration technique (GITT) and potentiostatic intermittent titration technique (PITT) (routinely carried out using composite electrodes) \cite{Wen01121979}\\
%Multi-scale measurements hel p resolve disputes in transport mechanism
\textbf{rate constants}: (microscale) reactive transmission electron microscopy (TEM)�\cite{Gu2012}; (mesoscale) electrolytic
solution ``design" \cite{Aurbach2004};  
(macroscale) electrochemical impedence spectroscopy (EIS) \cite{Meyers2000} \\
%(only ``effective reactivities" are of interest to the macro-models)  \\
%Multi-scale measurements help eliminate multiple �optima�
\textbf{open circuit potentials, entropy measurements, decomposition potentials}:  
(microscale) composition versus lattice structure (XRD) \cite{Ceder2009,Ohzuku1995};
(mesoscale) electrochemical quartz crystal microbalance (ECQM) \cite{Buttry1992};
(macroscale) C/50 discharge for free energy\\
%�Voltage fade�???, non-stoichiometric �phases�???, hysteresis!!!, decompn. potentials fairly well documented
\textbf{conductivities (electronic, ionic)}:  
(microscale) quantum transport, lithiation mechanisms \cite{Ceder2009};
(mesoscale) properties of individual components combined by porous media theory \cite{Stroud1975};
(macroscale) directly measure effective conductivities \\
%Not strictly multiscale; need to think about this more
\textbf{mechanical properties}: 
(microscale) in-situ TEM \cite{Wang:2012};
(mesoscale) single particle and in situ electrode level strain\cite{Qi:2010,Verbrugge:1999};
(macroscale) pressure transducers 
%Rational design across scales

Our goal is to develop the multiscale sampling techniques
needed to reduce and quantify uncertainty in LIB simulation.
Specifically, we will combine
macro-scale (cell level) experiments,
% atomistic calculations (quantum),
micro-scale (particle level), and meso-scale (electrode level) experiments to
understand the small scale--especially mesoscale (because this is where the
battery designer has the most degrees of freedom)--to 
understand the
origin of macroscale
pressure trends that correlate with battery degradation (e.g.,
capacity fade) and safety (e.g., thermal runaway, a.k.a. fire) issues.
%%%%%%%%%%% (end) batteries

\subsubsection*{Research issues}
The three application areas share a number of commonalities.
Each has a hierarchy of experiments that are used to probe the system.
All involve reaction and diffusion processes.
One characteristic of reaction diffusion processes is that some aspects of the system
may not be observable from the available data. For example, in a reaction chain the slowest
reactions dominate the response so that fast reactions are not effectively probed.
Furthermore, all of the applications
have a potentially complex relation between what is measured and quantity of interest,
even for relatively simple experiments. 
Planar laser induced fluorescence diagnostics in combustion, for example, 
measure photon emissions from excited states that have complex relation to the underlying
composition.
Furthermore, for the more complex experiments, the
quantities of interest are statistical / feature based
particularly at higher levels of the hierarchy.

In spite of the commonalities, the target application areas have a number
of distinguishing features.
Combustion is more well understood with mathematical models that are on firmer ground
with more sophisticated numerical models.
However, combustion models potentially have more parameters than the other two areas.
Models for photovoltaics are less well established and
model inconsistency is more likely to be an issue in this context.
Aside from model inconsistency, the bounds on the input parameters are typically less well known and much wider than in the combustion context. 
Photovoltaic models also have an inherently stochastic component
embedded in  the grain growth models.  
Batteries are similar to photovoltaics
in that the models are less sophisticated. However battery problems introduce a 
new element, namely that the problem is inherently multiscale.
In this case we need to devise mechanisms to communicate information
between different types of models at different scales.

\subsubsection*{Problem set up and overview of implicit sampling and MCMC}
An individual experiment with parameters $\theta$ might be modeled with deterministic dynamics as
\begin{equation}
\label{eq:ModelEquation}
x_t = L_e(x,t,\theta) \qquad \mathrm{with} \qquad x(0) = x_0 \; ,
\end{equation}
while for stochastic dynamics we have
\begin{equation}
\label{eq:ModelEquationSt}
dx = L_e(x,t,\theta) dt + dW \qquad \mathrm{with} \qquad x(0) = x_0 \; ,
\end{equation}
where $dW$ is a stochastic forcing term. 
%Uncertainty of the model is expressed by making the parameters $\theta$ random variables.
%For example, $\theta$ can be a Gaussian with mean $\mu$ and covariance matrix $\Sigma_0$.
%For each experiment we 
%assume we have an approximation for $\mu$ and $\Sigma_0$ that are known from prior investigations.
The initial conditions $x_0$ may be random as well; if so, we assume that we have a (prior) pdf to describe them.
We are also given data, $y_i$, at various time points $t_i$ in the evolution.
We assume that the data are functions of $x_i$ perturbed by observation noise:
\begin{equation}
	\label{eq:DataEquation}
	y_i = h_e(x_i)+v_i.
\end{equation}
For simplicity, we take the noise, $v_i$, to be independent Gaussian random variables with mean 
$0$ and covariance matrix $\Sigma$.
The observation function, $h_e(x)$, may involve a large loss of information.
For example, it may represent a 2D slice of a full 3D field.
Many data points are needed to get a sense of the full $x_i$.

One generic issue with Monte Carlo approaches
is the need for new algorithms to improve computational efficiency for large-scale problems.
Our target problems also include specific features that must be addressed.
Kinetic models for example often exhibit multiple reaction pathways where
many different parameter sets are nearly as good at explaining the data.
This type of phenomenon, referred to as model degeneracy,
causes isotropic sampling algorithms such as single variable heat bath (Gibbs sampler) or isotropic
Metropolis walk to slow down dramatically.
See \cite{HouStretch} for an illustration of ill conditioning arising in this way.
Another characteristic of our target applications is that
the exact model is not in the family being fit.
Models are only approximations to reality.
For example, the Arrhenius form for reaction rates are only modeling approximations, though they can be very accurate.
Similarly, models for diffusion are only approximations to the underlying molecular processes.
Even small modeling errors can make an accurate global fit impossible, particularly in chaotic systems.
This issue become more serious in the case of batteries and photovoltaic models where the models
are not yet mature.  In these areas we are likely to encounter structural issues where key physical
processes are missing from the description.
We need statistical methods that are robust to fundamental inconsistencies between models and data.
Finally, we need approaches that can deal with issues that arise from the highly nonlinear observation
functions, $h_e$, characteristic of the target applications.

The central theme of this project will be to develop new smart sampling technologies 
to address these issues.
The types of methods we plan to consider fall into two distinct types:  particle filter approaches
and MCMC.
MCMC creates a stochastic dynamics that samples the desired distribution.
As with iterative methods, the stochastic dynamics can be slow for ill conditioned problems.
The MCMC can be expensive if every MCMC step requires a forward simulation of the underlying model
(\ref{eq:ModelEquation}) or (\ref{eq:ModelEquationSt}).
For stochastic dynamics (\ref{eq:ModelEquationSt}), it is necessary to keep pieces of the 
stochastic trajectory $x_t$ as well as $\theta$ as part of the sample, which will make
memory efficiency an important issue for the hardest target applications.

Particle filters, arise in the context of data assimilation.
They are distinct from MCMC in that they produce independent samples.
They update a sample of the state $x_{i+1}$ and $\theta$ taking into account a new observation 
$y_{i+1}$.
They achieve independent samples using importance weights.
Particle filter methods for stochastic systems store only a single state $x_i$ rather than 
a trajectory, so they are more memory efficient.
The most common direct particle filter method uses weights that have extremely high variance, 
particularly when the observation noise, $v_i$,has small variance.
More recent implicit sampling particle filters, invented at LBNL
\cite{chorintupnas,chorin2010,Morzfeld2011,Morzfeld2012,Atkins2013}, address this high variance 
issue using a more sophisticated update/sampling strategy.
Each step involves an optimization, for which derivative/sensitivity information is very helpful.

It is not clear which of these approaches will be preferable in a given application, so
we need to understand the tradeoffs, e.g. between memory and computation
in each method to determine which approach will be most effective for a given experiment.
We anticipate that one approach is not optimal across a hierarchy of experiments, rather
that different approaches will be preferred based on specific problem characteristics. Intuitively,
we would expect MCMC to be a more attractive option for relatively simple problems with stochastic dynamics and a particle
filter to be a better choice for deterministic dynamics as problem complexity increases. Quantifying those relationships
and understanding how to transition between approaches are important research questions.
\MarginPar{RG: Maybe we should emphasize that
  defining an appropriate characterization of a model to choose which
  approach to use is a broadly useful outcome of our work}
Furthermore, although neither of these approaches is new, substantial development is needed to meet the requirements
of the applications we are considering.

We explain implicit sampling for the case of deterministic dynamics, known initial conditions and 
data at a single time, $T$.
In this case we can combine the model (\ref{eq:ModelEquation}) and data equation (\ref{eq:DataEquation}) to obtain
\begin{equation}
\label{eq:IS_data}
	y_T = H_e(x_0,T,\theta)+v,
\end{equation}
where $H_e$ is the function that is obtained by first running the model up
to time $T$ followed by applying $h_e$ to the state $x_T$. We are interested in the information we can extract from the data about the parameters $\theta$ and
therefore consider the random variable $\theta|y$, which is characterized by its pdf $p(\theta|y)$. Using Bayes' rule, we find that
\begin{equation}
\label{eq:post}
	p(\theta|y_T) \propto p(\theta)p(y_T|x_0,T,\theta),
\end{equation}
where  $p(\theta) = \mathcal{N}(\mu_0,\Sigma_0)$ is the ``prior'' and $p(y_T|x_0,T,\theta)$ is the ``likelihood'',
which can be read off of (\ref{eq:IS_data}), $p(y_T|x_0,T,\theta)\sim \mathcal{N}(H_e(x_0,\theta),\Sigma)$.
The pdf $p(\theta|y_T)$  is called the ``posterior'' and we wish to approximate it with implicit sampling. 

The general procedure is as follows. Define a function by
\begin{equation}
	F(\theta)= -\log \left(p(\theta)p(y_T|x_0,T,\theta)\right).
\end{equation}
Note that $F$ is a function of the uncertain parameters and that the minimizer of $F$ is the mode of the posterior. Thus, the high probability region of the posterior is the neighborhood of the minimizer of $F$ and we can identify this region via numerical minimization of $F$. To find samples in this region we solve (repeatedly) the algebraic equations
\begin{equation}
\label{eq:IS_sampling_eq}
	F(\theta)-\phi = \frac{1}{2}\xi^T\xi,
\end{equation}
where $\phi = \min F$ and $\xi$ is a Gaussian reference variable with mean zero and unit variance, i.e. the equations have a random RHS.
Note that the RHS is small with a high probability ($\xi$ has mean zero), which implies that the left hand side is also small with a high probability and, therefore, the solution is close to the minimizer of $F$ which is the mode of the posterior.
Thus, the solutions of these equations (for different realizations of $\xi$) are in the neighborhood of the mode of the posterior, i.e. almost all samples are compatible with the data.
Generating samples with low probability with respect to the data (as in standard MC sampling) is avoided by the minimization step. The weights of each sample are the Jacobian determinant of the mapping we choose to solve (\ref{eq:IS_sampling_eq}).


\MarginPar{JBB: Matti, it seems like we should say more about solving the random equations.\\ (matti): I added a few things. Is that enough?}
There are various ways for solving the algebraic equations of implicit sampling (\ref{eq:IS_sampling_eq}) \cite{chorin2010,Morzfeld2011}. 
For example, we can use information about the curvature of $F$, e.g. from BFGS-type optimization, for the construction of linear maps. Let $L$ be a square root of the Hessian of $F$ (e.g. Cholesky factor), then $\theta = \mu +L^{-1}\xi$, where $\mu$ is the minimizer of $F$, solves the quadratic equation $F_0-\phi=0.5\xi^T\xi$, where $F_0$ is the Taylor expansion of order $2$ of $F$. 
The error we make by solving a quadratic equation instead of (\ref{eq:IS_sampling_eq}) can be accounted for in the weights (without introducing bias). This method can fail if there is sufficient skew in the posterior pdf, or if the information about the curvature of $F$ is incomplete/inacurate (e.g. when derivative free optimization schemes are used). In this case, we can use the ``random map'' approach and pick a direction $\eta$ at random by setting $\theta = \mu+\lambda \eta$, where $\eta$ is the random direction and $\lambda$ is a scalar parameter that defines the sample. We search for solutions in this direction by solving a single algerbraic equation in a single variable (we must solve (\ref{eq:IS_sampling_eq}) for $\lambda$), e.g. by Newton iteration. The weights, i.e. the Jacobian of the random maps are straight forward to evaluate \cite{Morzfeld2011}. In summary, implicit sampling amounts to the following steps:
\begin{compactenum}
	\item Minimize the function $F$ to identify relevant regions of the parameter space
	\item Find samples in this region by solving the random algebraic equations (\ref{eq:IS_sampling_eq})\
        \item Compute weight of the sample
\end{compactenum}
While the general method of attack is clear, many research questions arise when applying this sampling technique to the target applications (see below for detail).

The other basic technique we plan to use is MCMC.
MCMC is basically a two step process.
\MarginPar{Jonathan please fix}
In a deterministic parameter estimatation mode, given the parameters $\theta_\tau$ at step $\tau$,
we first generate a proposal $\theta^*$ for new values of the parameters and compute
the probability of the posterior with respect to $\theta^*$ compared to $\theta_\tau$.
In the present setting this requires running a forward simulation.  We then apply an
acceptance criterion to either accept the new proposal and set $\theta_{\tau+1} = \theta^*$
or reject it and let set $\theta_{\tau+1} = \theta_\tau$.
When the dynamics is stochastic we include an estimation of the state in space and time, $X$,
in the process.
Although for high dimensional problems MCMC approaches are considerably more cost
effective than naive sampling approaches based on drawing independent proposals
from the prior, they can
exhibit a critical slowing down phenomenon for poorly conditioned problems.
The length of time that
one needs to simulate the Markov process to obtain good results depends on the autocorrelation time
of the underlying process, which can be extremely long for high dimensional systems.
Several approaches have been proposed to deal with this problem.
\MarginPar{JBB:  Jonathan check . . .i just made this up}
Here we will focus on multigrid MCMC \cite{Goodman1989}
and parallel marginalization \cite{Weare2007}.
Multigrid MCMC is similar in spirit to multigrid for a linear system.  One
defines a sequence of coarsened versions of the problem along with an effective
dynamics at the coarsened scale.\MarginPar{RG: How? ROM?}  One then performs what is essentially a standard
multigrid W-cycle except that instead of relaxation, one performs a simple sampling
approach such as a Gibbs sampler.  Under certain conditions, one can rigorously
prove that multigrid MCMC completely eliminates critical slowing down as the
system size increases.
Parallel marginalization is similar in spirit. It evolves a sequence of coarsened
version of the Markov process
simultaneously with the fine scale version of the process. 
The distributions for the coarsened processes are obtain as marginals of the original
distribution.  In addition to normal proposals within each process, we also consider
swap proposals between chains of adjacent resolution.  Proper choice of the acceptance
criteria for swaps guarantees that the overall combination of the processes converges
to the correct distribution and dramatically reduces correlation time.

\subsubsection*{UQ across a hierarchy of experiments}
A characteristic of our target applications is that there is a hierarchy of experiments.
How to use this characteristic for successful and efficient sampling is a major research question we will address.
In the data assimilation context in which particle filters originated, the algorithms move through a temporal sequence of data and sequentially update the state/parameters as data becomes available.
The update rule comes from repeated applications of Bayes' rule and ultimately leads to a recursive formulation of the posterior.
Here we need to modify the methodology to transition from one experiment to the next as we move through the hierarchy of experiments, i.e. we need to find the update rule for moving up in the hierarchy.
For example, we can construct priors at a higher level of the hierarchy from posteriors at lower levels. For implicit sampling this amounts to finding a representation of the posterior at a lower level that can serve as a prior in the optimization at the next stage.
Moreover, the theory and numerics can be intertwined here because the models obtained (as posteriors) at lower levels of the hierarchy may help with speeding up the minimizations required at higher levels of the hierarchy (see below for more detail). 
Another central issue is determining whether or not moving through the hierarchy of experiments from the simplest to the most complex introduces bias in the parameter estimates. If this bias is found to be significant, strategies must be developed to reduce that bias. 

\subsubsection*{Model degeneracy and model inconstancies}
Another key problem that has to be addressed is model degeneracy.
A model has an approximate degeneracy if many different parameter sets are nearly as good at explaining the data.
For example, if there are multiple reaction pathways in a kinetic description,
certain combinations of reaction rates may be much better
estimated than the individual rates. Model degeneracies can lead to difficulties with implicit sampling, since these correspond to ``valleys'' in  the function $F$.
The valleys slow down the convergence of the required optimizations and, more importantly, a good approximation of the degeneracy requires the valleys being populated with samples which will increase the number of samples one needs to generate.
The computational expense of implicit sampling will thus increase.
To fix these issues, we will investigate how to connect MCMC to implicit sampling to speed up exploration of model degeneracies.
However, current MCMC algorithms, e.g. isotropic sampling algorithms such as single variable heat bath (Gibbs sampler) or isotropic Metropolis walk will also be slow when model degeneracies occurs.
We plan to use the affine sampling approaches developed at NYU to address issues of model degeneracy \cite{GoodmanWeare2010}.
The idea here is to use an ensemble of $N$ samplers instead of just one. 
If $N$ is sufficiently large, an ensemble sampler can be constructed that is affine invariant; i.e., it leaves the ensemble
random walk unchanged under affine transformation.  This property guarantees that the method will work well for problems that can
be rescaled to be well conditioned with an affine transformation.
Although $N$ can be quite large for a large system, we conjecture that $N$ of the order of the length of the reaction pathways
in the system should eliminate most of the model degeneracy.
\MarginPar{(matti): Jonathan, does this make sense?}
\MarginPar{RG: also possible that model is inconsistent with data
  given prior. In this case, we want to extract information to help
  design a better experiment to choose between equivalent parameter combinations}

As noted above, it is inevitable that the exact model is not in the family being fit and
the statistical methods we develop must be robust to this situation.
%For example, the Arrhenius form for reaction rates are only modeling approximations,
%though they can be very accurate. Similarly,
%typical models for species diffusion in combustion are only approximations, even at the continuum level,
%to a full transport model, which is, in itself, an approximation to the underlying molecular processes.
%Even small modeling errors can make an accurate global fit impossible, particularly in chaotic systems.
%A more serious structural issue arises in the case of batteries and photovoltaic models where the models
%are not yet mature.  In these areas we are likely to encounter structural models where key physical
%processes are missing from the description.
One approach to address this type of model inconsistency 
is to include noise in the dynamics, so that the posterior distribution does not require the
dynamical equations to be satisfied exactly.
This increases the complexity of both MCMC and implicit sampling approaches. In both cases, the sampler would 
include estimation of the entire state in space and time.
An interesting intermediate position is to only introduce stochastic perturbations periodically.  The system
can be deterministic evolved between perturbations. This allows
us to have some tradeoff between computation and memory.
Balancing that tradeoff with architectural features and fidelity of the algorithms needs to be explored.
We will conduct computational experiments to study this problem, then use the results to 
choose appropriate noise levels for our physical models.
By combining what is known about the error levels in both model and data, we can also estimate upper and lower bounds
of the predictive skills of the resulting stochastic models.
\MarginPar{can be develop criteria to assess if something is missing form model}

\subsubsection*{Efficient sampling in view of complex observation operators}
%Another feature of all three target applications is a complex relation between the data and the parameters being estimated. For example, laser diagnostic measurements depend on the system state but there are parameters describing the laser interaction (so-called quenching coefficients) that have uncertainties as well, i.e. the observation function $h$ itself can be uncertain. We plan on extending the implicit formalism to estimate uncertainties in the measurement mechanisms. 
Another feature of all three target applications is a complex relation between the data and the parameters being estimated.
\MarginPar{JBB:  made this more specific . . .perhaps too much. . .left original text}
For example, in
laser diagnostic measurements of a species S,
the intensity, $I_{{LIF}}$,
for weak, non-perturbing laser excitation is given by
\begin{equation}
I_{{LIF}} \; = \; c_{cal} \; I_{{laser}} \; N_{\rm S} \sum_i 
f_{B,i}
(T) \; B_{i,k} \; g_{\lambda,i} (p, T, X) \sum_{k,j} {A_{k,j} \over \sum_\ell A_{k,\ell} + Q_k (p
,
T, X)} \; .
\label{eq:quench}
\end{equation}
Thus the measured signal
depends on the number density of
the excitable molecules (number density $N_{\rm S}$
times the Boltzmann fraction $f_{B,i}$), the Einstein coefficient
$B_{i,k}$ for
absorption $i \rightarrow k$, the spectral overlap fraction
$g_{\lambda,i} (p, T, X)$ of the laser profile, the
absorption spectrum of species S, and the fluorescence quantum yield
$A/(\sum A + Q)$, where $A$ and $Q$ are decay rates due to
spontaneous emission and electronic quenching, respectively.
Although
equation (\ref{eq:quench}) provides a quantitative relationship
between the measured density of S and laser intensity, 
the uncertainties in the various terms introduce additional sources of uncertainty
into the measurement function $h_e$.

We plan on extending the implicit formalism to estimate uncertainties in the measurement mechanisms.
In addition, the actual measured quantity is
an extremely complex function of the state.
This type of complex relation can impact the choice of mapping from $\xi$ to $x$ in the implicit sampling algorithm,
We can investigate
this issue by using synthetic experiments that transition from viewing the data
as a simple projection of the state through increasing levels of complexity up to
a computational model of the actual measurement.  From this type of experiment we can 
analyze how different aspects of the measurement alter the information we can
obtain from the measurement. This type of analysis will also allow us to assess the
role of noise in highly nonlinear observations of the state.

At high levels of the hierarchy in turbulent combustion, we are interested in finding parameters that
consistent with observed features in the data that are insensitive to details of the state trajectory.
In fact, we want to avoid trying to estimate a trajectory that is a point-wise fit to the trajectory
given by a measurement because it is a hopeless task.
\MarginPar{JBB:  is this too hopeless? (matti) I hope not, I would like to try.}
On an abstract level, this corresponds to the problem of translating qualitative behavior observed in the data
into quantitative information about parameters (by using sampling techniques) and, to the best of our knowledge,
this fundamental problem has not been addressed before.
We anticipate that tackling this problem will require a careful re-evaluation and perhaps re-definition of the observation function $h_e$
and we plan to take first steps towards a methodological and quantitative assimilation of data features. 
A major hurdle here is that we cannot use adjoints for the optimization needed by the implicit sampling approach.  Instead we will need
some type of surrogate representation of the likelihood to perform the optimization.

\subsubsection*{Multi scale sampling}
We plan to extend our method to multi-scale systems by addressing
\MarginPar{at this point this is sampling only ... how would MCMC work. . Jonathan? I tried to disconnect the first part of the discussion from IS. This way we can talk about multi scale IS and MCMC}
how information can be propagated efficiently across different scales
using multiscale battery simulation as a case study.
Specifically, we are interested in the fluctuation of pressure with time since it can critically affect performance, degradation, and safety of batteries.
This ``macro scale''(millimeters)  pressure is the manifestation of many effects at the ``mesoscale''(microns):
conversion of electrolyte to the gas phase; the sponge like nature of the porous electrode; elastic properties of the binder (the binder is the ``glue'' that holds the particles in their porous structure); the porosity and tortuosity of the separater.
The pressure is the sum of these effects combined across the whole cell and can be measured via detailed signals from transducers.
In addition, we have data at the mesoscale such as stress, strain, swelling, and other mechanical properties.
Our goal is to incorporate all available data to form an estimate of the pressure and the mesoscale state,
described in terms of layer thicknesses (substrate, epoxy layer, current collector, active material, entire electrode),
volume fractions (particles, binder, pores), porosity, tortuosity, particle sizes and distributions (see, e.g., \cite{Sethuraman2012334})
and other parameters governing the makeup and morphology of the heterogeneous electrode.
By using Bayes' rule we can factorize the conditional pdf of the cell-level pressure and electrode-level stress and strain, given both
cell and electrode level data:
\begin{equation}
\label{eq:MultiScalePDF}
	p(x^1, x^2|y^1,y^2) \propto p(y^1|x^1) p(y^2|x^2) p(x^2|x^1)p(x^1),
\end{equation}
where we use the indices to distinguish scales, i.e. $x^1,y^1$ correspond to the mesoscale state and data, $x^2,y^2$ to the macro scale state and data.
Here, the first two terms come from a model of how the data is connected to the state (at each scale); the third term describes how the various mesoscale quantities affect pressure and the last term is the prior for the mesoscale parameters.
The above equation demonstrates how both the data and physical models are being incorporated and combined across scales.
By switching the indices, we can use this formalism to push information from macro- \emph{to} mesoscales, which may be important since macroscale battery measurements are typically \emph{more} reliable than mesoscale measurements. In this case, the physical model (with switched indices, now $p(x^1|x^2)$) acts as a constraint
(what $x^1$ is compatible with $x^2$?) rather than as a prediction (what is $x^2$, given $x^1$?) because
of the many-to-one relation between mesoscale parameters and macroscopic pressure.
A research question is to assess the validity of the assumptions that lead to a factorization of the pdf,
in particular we need to verify that the scale coupling models are (nearly) ``Markovian'', i.e.
%they couple only two subsequent scales, and
that the observation models at each scale only depend on the state at that scale. 

A successful and efficient solution of this multi scale problem allows physics-based models (e.g. how pressure arises from mesoscale phenomena) to inform our interpretation of macroscale measurements, or, conversely, allows the physics-based model to ``learn" from the macroscale measurements.
We plan to develop a multi-scale version of implicit sampling to approximate the above (multiscale) pdf (\ref{eq:MultiScalePDF}) and, in particular, we will address implementation issues, e.g. how will the coupling between scales affect the required optimizations and subsequent solves of~(\ref{eq:IS_sampling_eq}).
We expect that the required computations will be extreme scale, since the problem requires resolving and coupling of phenomena that vary over many orders of magnitude.

\subsubsection*{Implementation, numerics and extreme scale computation}
There are also many implementation issues that to need be addressed, especially with respect to efficient scaling of the MC algorithms on massively parallel computers.
In implicit sampling the minimization is the computational bottleneck and its efficient implementation is crucial for the success of the method.
Moreover, the minimization algorithm will likely depend on where we are in the hierarchy of experiments.
One of our specific research topics will be to address efficient minimization at each level of the hierarchy, especially in view of massively parallel computer architectures.
For example, we can consider coupling adjoint codes for gradient computations of $F$ to BFGS-type algorithms using a parallel optimizer (e.g., TAO from Argonne).
At high levels of the hierarchy, adjoint codes may be out of reach or too costly and time consuming to construct.
In these cases, derivative free optimization methods must be considered.
Another possibility is to use simplified models for the minimization.
For example, we can borrow ideas from multi-grid and run the minimizations on a coarse grid, while doing e.g. forecasting on the fine grid.
Alternatively, we can use a surrogate method, where a small number of forward simulations are used to generate a simplified model of the simulation.
This model is then used in the optimization and its further refinement goes hand in hand with its use in seeking the minimum.
Many research questions surround the interplay between optimization and sampling, particularly with approximate surrogate models.
For example, we can use numerical experiments to find a characterization of how errors introduced by a simplified model impact the overall behavior of the algorithm and what must be known about the errors of the simplified model to obtain such a characterization.  

We can reduce the complexity of the response that reduced order models are required to emulate by modeling the difference between the simple and the complex models. A promising approach that is consistent with~(\ref{eq:IS_data}) is GPR. 
Consider outputs from two different models, $H^S$ and $H^C$ where $H^C$ is deemed more accurate than $H^S$. 
Then we can write (\ref{eq:IS_data}) as 
\begin{equation}
y(\theta) = H^S(\theta) + (H^C(\theta) - H^S(\theta)) + v,
\end{equation}
and GPR can be used to model  $(H^C - H^S)$ as $\mathcal{N}(m_{\rm GPR}(\theta;\bar{\theta}),\Sigma_{\rm GPR}(\theta;\bar{\theta}))$, where $\bar{\theta}$ are sample points used to construct the GPR model. Since both $m_{\rm GPR}$ and $\Sigma_{\rm GPR}$ depend on $\theta$, this approach is capable of modeling the nonlinearity in the difference. Application of implicit sampling (see above) to this  formulation seems feasible, however appropriate models for $m_{\rm GPR}$ and $\Sigma_{\rm GPR}$ and their construction procedures that efficiently utilize extreme scale computers are research questions that need to be answered. 

%This type of reduced-order models can also play a role in improving the efficiency of MCMC algorithms.  Several of the techniques for improving MCMC involve
%\MarginPar{JBB:  added this . . too tired to decide if it is just bs.  Jonathan?}
%evolving coarser changes with algorithms for sharing information between those chains.  Typically the relationship between coarse and fine chains
%correspond to simple averaging and sampling procedures.  However, we can potentially consider a much wider range of ``coarse'' models where a reduced-order model
%is used to map between ``coarse'' and fine levels.
%
%\MarginPar{JG doubts this will work and notes a need to for derivative information to guide sampling. JBB
%doubts we can perform adjoint simulations for 3 turbulent simulations.  Only way out JB can think of
%is based on discussion with George about combining coarse simulation with a statistical surrogate to
%build an estimate of finer response . . . Unless someone has a brilliant idea here i suggest we leave this
%for the preproposal and thing of how to address it in real proposal}
%\MarginPar{need to decide if there is something viable here. Another alternative that merits consideration
%is using the adjoint of a simpler model for the optimization.}

Several other practical issues will be addressed with both implicit sampling and MCMC.
For any given experiment, we may have access to data at more than one time and we may have more than one type of data.
There are two options for using these data for estimation:
(\emph{i}) we can extend the
%implicit
sampling formalism to include all $m$ data sets and estimate the parameters using all the data (off-line estimation); or
(\emph{ii}) we can estimate the parameters using a batch of $k_1$ data sets and then refine this estimate using
the remaining data in batches of $k_n$ sets, i.e. we can move through the data sequentially (on-line estimation).
Theoretically, off-line estimation seems more attractive, since more data should lead to more accurate estimates because it avoids bias (e.g. the maximum likelihood estimator is asymptotically unbiased as the amount of data goes to infinity).
However in practice one often finds an ``optimal'' number of data sets per estimation sweep (i.e. an optimal $k$), the reason being (at least in part) that the exact model is not in the family being fit (even at high levels of  accuracy).
This is used in numerical weather prediction and we plan to investigate the optimal number of data sets per estimation sweep for our target applications. 

In implicit sampling, we may also need to relax the Gaussian assumptions for the prior and likelihood.
For example, a Gaussian prior is not meaningful if the parameter is known to be positive. Similarly, we have chosen a Gaussian ``reference variable'' $\xi$, but other choices are also possible. We plan to investigate the interplay between priors, likelihood functions and the reference variable in implicit sampling and will determine good choices for reference variables for each target application and at each level of the hierarchy.

\subsection*{Timetable of Activities}

The investigation of the research questions posed above will be pursued in the context of examples drawn from our target applications.
We will initially focus on relatively simple models and progress to more complex systems as the methodology matures.
Specific timetable for the research activities are given below.
Personnal are denoted by initials with $GS$ representing the graduate student at NYU. Levels of effort are given in terms of FTE.
\MarginPar{needs to add to jbb .25, msd .25, gp .2, rg .4, pg .4, jg .08, gs .75, feel free to modify. }
%

\underline{First Year}
\begin{compactitem}
\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}\setlength{\parsep}{0pt}
\item Implement implicit sampling for 1D time dependent flames with detailed chemistry using finite difference derivatives for optimization
(JB 0.1; MM 0.1)
\item Implement adjoint code to 1D flame solver to improve optimization for implicit sampling
(MD 0.1; MM 0.1)
\item Implement MCMC for 1D flame models and test multigrid MCMC and parallel marginalization
(JB 0.15; JG 0.08; GS 0.45)
\item Develop 1D and 2D adjoint model for battery simulation at electrode and cell scales
(PG 0.2)
\item Evaluate use of multiscale implicit sampling for 1D and 2D battery model
(PG 0.2; MM 0.3)
\item Implement MCMC for 1D version of stochastic point defect model
(RG 0.2; GS 0.3)
\item Investigate role of nonlinearity and uncertainty in measurement using synthetic PLIF
(MD 0.15; RG 0.2)
\item Investigate use of ROMs to relate flame simulations at different resolutions
(GP 0.2)
\end{compactitem}
%
\underline{Second Year}
\begin{compactitem}
\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}\setlength{\parsep}{0pt}
\item Extend implict sampling to 2D vortex flame interaction
(MD 0.15; MM 0.2)
\item Investigate use of surrogate in conjunction with coarse models to speed up optimization
(GP 0.2; PG 0.2)
\item Investigate incorporating noise into MCMC to address model inconsistency
(JB 0.15; JG 0.04; GS 0.45)
\item Investigate use of affine sampling for model degeneracy using combustion with detailed kinetics
(MD 0.1; JG 0.04; RG 0.2)
\item Extend MCMC for stochastic point defect model to 2D
(RG 0.2; GS 0.3)
\item Test hierarchical sampling multiple experiments for battery models
(PG 0.2; MM 0.2)
\item Formulate feature-based sampling strategy for simplified chaotic system
(JB 0.1; MM 0.1)
\end{compactitem}
%
\underline{Third Year}
\begin{compactitem}
\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}\setlength{\parsep}{0pt}
\item Test hierarchical sampling for sequence of combustion experiments
(JB: 0.1; MD 0.25; JG 0.04; MM 0.1)
\item Test hierarchical sampling for photovoltaic point defects
(RG 0.4; JG 0.04; GS 0.75)
\item Test hierarchical sampling for battery models
(PG 0.4; MM 0.1)
\item Test feature-based sampling on turbulent flame simulation
(JB 0.15; MM 0.2)
\item Evaluate use of ROM to map uncertainties
(GP 0.2)
\end{compactitem}

\subsection*{Project Objectives}

The goal of this project is to develop a mathematical framework  
based on novel sampling methods that
intertwines parameter estimation and simulation 
to estimate uncertainty and improve prediction for target systems.
Specifically, we want to
(1) use available data from a hierarchy
of experiments of increasing scale and complexity to restrict
uncertainty in the description of the system, (2) estimate the impact of the improved characterization
on predictive capability and (3) identify which of the remaining uncertainties have the most impact
on the uncertainty of predictions.
We will demonstrate the use of the framework for prototype problems in combustion,
novel photovoltaic material and lithium-ion batteries.


\bibliographystyle{abbrv}

\bibliography{george_rom,pd,batteries3,jg} 

\end{document}
