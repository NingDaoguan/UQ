\documentclass[11pt]{article}
% RFP specifically says to use 11 point type and 1 inch margins
\usepackage{graphicx}
\usepackage{epsf,color}
\textwidth=6.5in\oddsidemargin=0in \evensidemargin=0in \topmargin
0pt \advance \topmargin by -\headheight \advance \topmargin by
-\headsep \textheight 9.0in

%\textwidth=6.5in\oddsidemargin=0in \evensidemargin=0in \topmargin
%0pt \advance \topmargin by -\headheight \advance \topmargin by
%-\headsep \textheight 8.9in

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage[compact]{titlesec}

%\usepackage[plain]{fullpage}
\usepackage{amsfonts}
%\usepackage{lastpage}
%\usepackage{fancyhdr}

%\usepackage[version=3]{mhchem} 
% you can use this command to skip chunks of your document
% just put the command around the chunk like this
% \comment{ ...the chunk... }
\newcommand{\comment}[1]{}

%\newcommand{\MarginPar}[1]{\hspace{1sp}\marginpar{\tiny\sffamily\raggedright\hspace{1sp}#1}}
\setlength{\marginparwidth}{0.75in}
\newcommand{\MarginPar}[1]{\marginpar{%
\vskip-\baselineskip %raise the marginpar a bit
\raggedright\tiny\sffamily
\hrule\smallskip{\color{red}#1}\par\smallskip\hrule}}

%\renewcommand{\baselinestretch}{1.05} % = 1.0 Single space; = 2.0 Double
\renewcommand{\baselinestretch}{1.0} % = 1.0 Single space; = 2.0 Double

%\renewcommand{\refname}{Literature Cited}
%------------------------

%\pagestyle{empty}  % No page numbers
%\textfloatsep 0mm
%\abovecaptionskip 1mm

\begin{document}

%\pagestyle{plain}
%\pagenumbering{roman}
\begin{center}
{\large{\textbf{A Hierarchical Approach to Uncertainty Quantification}}}
\end{center}

\subsection*{Background / Introduction}

Many important DOE applications rely on simulation to predict the behavior of complex physical systems.
However, the fidelity of these simulations depends on uncertain parameters describing the underlying physical system
obtained from a collection of
complex and noisy experiments whose reliability is hard to determine. 
Our ability to effectively use extreme scale computing will depend critically on our ability to reduce the
uncertainty in these data and assess the impact of that uncertainty on predictive capability.
Here, we will focus on combustion modeling, battery simulation and design of high-efficiency photovoltaic
devices as motivating examples.
\MarginPar{a bit more detail here?}
However, the methodology will be broadly applicable to a
range of problems in chemical and materials science, systems biology, subsurface flow and climate to name a
few.

An important aspect of the class of problems we plan to consider is that 
the system is not probed using a single experiment.  Instead,
there is typically a hierarchy
of experiments of increasing complexity that provide information about the underlying processes
that lead up to the desired target application.
As experimental complexity increases the fraction of the state that can be sampled
by measurement is reduced and the cost of simulations increases.
Rather than attempting to estimate parameters directly from a single complex experiment, we 
will obtain parameter estimates across the entire hierarchy of experiments.
Thus, we need an approach that allows us to pass
information through the hierarchy in a way that effectively uses data from all levels
to improve overall predictive properties.
Furthermore, as complexity increases,
the combination of rich physics and relative data sparsity suggests that we will not be
able to exactly match the experimental dynamics computationally.
We must develop estimation methods that are robust to model errors as well as noisy observations
using metrics based on identification of characteristic features that are more general than
traditional quantities of interest.

The goal of this project is to develop a
mathematical framework for this class of problems that
will utilize data from a hierarchy of experiments of increasing complexity to reduce
uncertainty of simulations, estimate the impact of the improved characteristic on predictive capability
and identify significant remaining sources of uncertainty.
%intertwine parameter estimation and simulation into an integrated activity.
Our approach is based on novel Monte Carlo (MC) sampling approaches within a Bayesian UQ framework.  \MarginPar{Matti: I wanted to bring up what the posterior is and moved the nonlinearity part, because we respect the nonlinearity by doing sampling rather than linearizations as in Kalman filters/least squares. }
The Bayesian approach combines modeling and sampling in a way that provides full insight into the propagation of 
uncertainty. The result is a complete characterization of the posterior probability density function (pdf) which contains all information about remaining uncertainties that is implied by the data -- which aspects are tightly bounded and which are less precise. An implementation of the Bayesian approach via MC sampling respects the strong nonlinearities in the target applications and avoids the need for additional approximations and simplifying assumptions 
required by some current UQ techniques. In addition, some MC sampling methods are well suited to massively parallel computer architectures because \MarginPar{Matti: is this correct? Exascale requires little communication, which is why I thought we could bring this up?}
the computationally expensive calculations, e.g. forward or adjoint model runs, can be executed independently and the communication between cores for each independent run is small.
Using MC sampling as the computational backbone will lead to a numerically sound implementation of a rigorous UQ theory
that is well suited for future (exascale) machines, making UQ possible for realistic applications.

As we move through the hierarchy of experiments, different MC sampling techniques can be used since the Bayesian approach is conceptually independent of its implementation. In particular, we plan to consider using both particle filter (importance sampling) methods and Markov chain Monte Carlo (MCMC).\MarginPar{Matti: started review for sampling} The key issue with both is that many  of their current implementations are not robustly effective for complex, high-dimensional problems. In particular, standard particle filters first obtain samples of the prior and then evaluate their probability with respect to the posterior by attaching weights to each sample based on its distance to the data \cite{Doucet2001,GordonSIR}. The weighted particles form an empirical estimate of the posterior. Difficulties arise if the prior and posterior approach being mutually singular, i.e. if high-probability events with respect to the prior are likely to have low probability with respect to the posterior. This is the interesting case, which corresponds to the situation where one can learn from the data, rather than simply confirming the prior. However, in this case many of the samples have a low weight so that the empirical estimate they constitute is poor (it can be a single and often unlikely point). This can happen in a single dimension, however it was shown rigorously for linear problems that the number of particles required scales catastrophically with the dimension of the problem \cite{Bickel,BickelBootstrap,Bickel2,Snyder,Weare2012,Weare2009}, making this approach inapplicable to realistic problems. A number of methods have been invented to ameliorate this problem, most of which amount to finding a proposal density that  generates samples that are more compatible with the data \cite{Doucet,OptimalImportanceFunction,liuchen1995,Brad}. For example, a nudging technique was proposed in meteorology, however the method is not theoretically sound, requires a lot of ad hoc tuning  and is not easily applicable to parameter estimation \cite{vanLeeuwen}. A different approach is to construct a map that transforms the prior measure into the posterior measure, i.e. to construct a map that transforms prior samples into posterior samples \cite{Moselhy2013}. The advantage of this method is that posterior samples are easy to generate once the map is found. However, the construction of the map is theoretically and computationally expensive and, therefore, relies on a number of approximations (e.g. polynomial representations of the maps). We expect that we will need to compute a map at each level of the hierarchy and, thus, expect that this approach is computationally too expensive for our target applications. Moreover, the errors of the approximations involved in finding the map are, at this time, not well understood. 

We plan to develop particle filtering methods based on the implicit sampling methodology invented at LBNL \cite{chorintupnas,chorin2010,Morzfeld2011,Morzfeld2012,Atkins2013}. The central idea here is to first search for the regions of high probability with respect to the posterior. This search can be implemented by numerical optimization. Once the high probability region is located, samples that lie within this region can be generated by solving simple algebraic equations. Although the optimization is computationally expensive, implicit sampling was shown to outperform standard MC importance sampling in small geophysical models \cite{Morzfeld2011,Morzfeld2012,Atkins2013}.  However, for the realistic applications we are considering, substantial development is needed, especially with respect to efficient use of massively parallel computer architectures. 

\MarginPar{Matti: here we start talking about a ``hierarchy of models'', but before we only talked about a hierarchy of experiments. Should the hierarchy of models be mentioned earlier (perhaps as a feature of the applications)? Or is my understanding of  ``experiment'' too narrow?}
\MarginPar{George: started review of ROM in UQ.  Will add more on uncertainty quantification of prediction and sensitivity analysis} The presence of a hierarchy of computational models that correspond to the hierarchy of experiments can be further exploited to improve efficiency of these sampling techniques. Approaches that involve multi-resolution models include importance sampling based on posterior distribution generated using coarser (thus cheaper) models~\cite{Higdon:2002vx,Christen:2005wp,Efendiev:2007uw,Bal:2013tp} and metropolis coupled chain~\cite{Higdon:2002vx}.   To further improve the match between the coarse and fine posterior distributions, Bal et. al.~\cite{Bal:2013tp} used a zero-mean Gaussian model to approximate the discretization error~\cite{Kaipio:2007ux}. However, since our simpler models may involve simplification of physics or reduction of spatial dimensions, a more accurate approach to approximate the modeling error is needed.  We intend to examine how we can use reduced order modeling techniques to model this error accurately.  In particular, Gaussian process regression~\cite{Rasmussen:2006vz} is expected to fit within the statistical framework that we will describe. We note that reduced order models can also be directly used as the simpler model~\cite{BuiThanh:2012tx}. For implicit sampling, reduced order models have the potential of reducing the cost of the required optimizations, either through the use of the simpler models in the optimizations or  via more direct optimization algorithms that utilize reduced order models~\cite{Regis:2007,Wild:2011uh}.

Another important aspect of uncertainty quantification is forward uncertainty propagation, including uncertainty quantification of prediction made using computational models and sensitivity analysis used in decision making and experimental design.  Existing research shows that the use of multilevel Monte Carlo approach can improve the convergence rate for a series of convergent models~\cite{Giles:2008gc}.  However, theoretical framework for a heterogeneous set of models are lacking.  Another approach is to use as efficient surrogates for fine-scale models in these analyses~\cite{Challenor:2012uv, Ratto:2012tf}.  Large number of techniques are available and a research question is how reduced order models can be most accurately used in a hierarchical framework. 


% Although efficiency Monte Carlo of sampling techniques has been significantly improved in recent years, many problems are still computationally intractable, even with the computational gain achieved through high-performance computing efforts (Wang et al., 2011). One feasible solution is to use reduced-order models (ROMs), or emulators, as efficient surrogates for fine-scale models in these analyses (Challenor, 2012; Ratto et al., 2012).  




Metropolis and heat bath algorithms suffer on problems that are poorly conditioned.
\MarginPar{Turn this into literature review?}

\subsection*{Proposed Research}

To frame the discussion of the proposed research we need to first provide some specific detail
about the target applications.
Simulating the combustion of a given fuel relies on parameters
describing chemistry, transport and thermodynamics.
Here we will assume that the multicomponent reacting Navier Stokes equations with Arrhenius
kinetics and a molecular transport model provide a good approximation to the dynamics.
The target question we would like to answer is how confident are we in the computed statistical
properties of a turbulent flame simulation and to what extent are the simulations consistent
with experimental data.
However, a direct attack on this problem is infeasible.
The computational complexity is sufficiently high and the available data is sufficiently
sparse that very little can be determined by directly pursuing the final target.
A way to get around this is to note that one has not simply a single turbulent flame
experiment but a wide range of different types of experiments for the same physical
system of increasing complexity.  
\MarginPar{something about the type of data somewhere in here}
Hierarchical data sources (experiments): 0d ignition, 1d laminar flames, shock-tube experiments
a variety of 2D laminar flames, and 3D turbulent flames.
One could potentially augment this with other types of non-reacting flow experiments that focus
on transport properties. 
Each of these experiments reveal information about the system (some are used by kineticists already).
The idea would be to use experimental data across the entire hierarchy of experiments to reduce
uncertainty in parameters and estimate bounds on how those uncertainties impact predictive capability.

Our approach needs to reflect the notion that as we move up the hierarchy, we are
getting closer to the target application but the simulations become
increasingly costly and the data become increasingly sparse.
The approach also needs to  respect the structure of the problem.
A reasonable cartoon for a 
turbulent flame experiment is that it corresponds to a stationary chaotic dynamical system.
What the experimentalist is capturing are some level snapshots of what the attractor looks like.
We cannot hope to have a complete picture of the flame at a given time.  We can only
have some characterization of selected features.  The available data is at best limited;
however, the hope is that it carries some information about the underlying physical system. \MarginPar{Matti: is this a research problem? I am not aware of anybody doing "qualitative assimilation". Should we expand this?}

DISCUSSION OF OTHER PROBLEMS -- NEED EQUATIONS HERE I SUSPECT

The three application areas share a number of commonalities.
Each has a hierarchy of experiments that are used to probe the system.
All involve reaction and diffusion processes in fluids.
One characteristic of reaction diffusion processes is that some aspects of the system
may not be observable from the available data. For example, in a reaction chain the slowest
reactions dominate the response so that fast reactions are not effectively probed.
Furthermore, all of the applications.
have a potentially complex relation between what is measured and quantity of interest,
even for relatively simple experiments. 
Planar laser induced fluorescence diagnostics in combustion, for example, 
measure photon emissions from excited states that have complex relation to the underlying
composition.
Furthermore, for the more complex experiments, the
quantities of interest are statistical / feature based
particularly at higher levels of the hierarchy.
\MarginPar{from discussion of 5/13 this my be too hard. Matti: see above. What do we do about this?}

In spite of the differences, the target application areas have a number
of distinct features.
Combustion is more well understood but with mathematical models are perhaps on firmer ground
and numerical models are perhaps more sophisticated
However, models potentially have more parameters than the other two areas.
Models for photovoltaics are less well established.
For this type of problem 
model inconsistency is a more likely issue in this context.
Batteries are similar to photovoltaics
in that the models are less sophisticated. However battery problems introduce a 
new element, namely that the problem is multiscale.
In this case we need to devise mechanisms to communicate information
between different types of models at different scales.

The central theme of this project will be to develop new smart sampling technologies 
to address these issues.
The types of methods we plan to consider fall into two distinct types:  particle filter approaches
and MCMC.
Both of these approaches aim to reduce the computational work associated with naive MC approaches.
Although they use different strategies, they both seek to avoid simply sampling the prior and evaluating
the posterior probability of the resulting sample. 
MCMC algorithms use the trajectory of the simulation in space and time to define the state for a Markov
process that samples the underlying Gibbs distribution of the problem.  Evolution of this Markov process
effectively samples the state space of the system.  The need to represent the full state of the system
make MCMC a memory intensive task.
Particle filters, which arise in the context of data assimilation,
fit into a framework of more traditional Monte Carlo algorithms.  Here the goal is to use
derivative / sensitivity information to guide the selection of samples to reduce the number of samples
needed to effectively sample the distribution.
Compared to MCMC, the filtering approach is less memory intensive but more computationall intensive, thus representing
a tradeoff between memory and computation.
We need to understand these tradeoffs to determine which approach will be most effective for a given
experiment.  We anticipate that one approach is not optimal across a hierarchy of experiments, rather
that different approaches will be preferred based on specific problem characteristics.  Intuitively,
we would expect MCMC to be a more attractive option for relatively simple problems and a particle
filter to be a better choice as problem complexity increases. Quantifying those relationships
and understanding how to transition between approaches are important research questions.
Furthermore, neither of these approaches is new; however, substantial development is needed to meet the requirements
of the realistic applications we are considering.

The particle filter methodology we plan to develop is based on the implicit sampling methodology developed at LBNL.
The central idea is to avoid prior sampling by finding a probability distribution that better approximates the
relationship between parameters and data.
\MarginPar{need equations to introduce the sampling thing and how it works. Matti: I started doing that}

Specifically, suppose we have  a  model of the physical process under consideration (e.g. a discretization of multicomponent reactive Navier Stokes) that maps the state at time $t=0$, $x_0$, to the state at time $t=T$, $x_T$, and suppose that this model includes a number of parameters $\theta$. We respect the uncertainty of the model by making the parameters random variables. For example, $\theta$ can be a Gaussian random variable with mean $\mu$ and covariance matrix $\Sigma_0$. We assume that $\mu$ and $\Sigma$ are known from prior investigations. We further assume that the initial conditions $x_0$ are known precisely. This assumption is reasonable in some applications, e.g. in combustion, where the initial conditions are well known, at least at some levels of the hierarchy. In other cases, the initial conditions are parameterized by a relatively small number of (uncertain) parameters, e.g. in battery simulation, where the initial conditions are characterized by the initial voltage of the macroscopic cell. In these cases, we lump the uncertainty of $x_0$ into the random vector $\theta$, i.e. we allow for $x_0 =x_0(\theta)$ but do not highlight this (possible) dependency in what follows. \MarginPar{Matti: is this correct?} Denoting the model as $M$, we have
\begin{equation}
	x_T = M(x_0,\theta, T).
\end{equation}
The data, $y$, is a function of $x_T$ and we assume that the measurements are perturbed by noise, which, for simplicity, we assume to be Gaussian:
\begin{equation}
	y = h(x_T)+v,
\end{equation}
where $v$ is a Gaussian random variable with mean $0$ and covariance matrix $\Sigma$. Since $x_T$ is a function of $x_0$, $T$ and $\theta$, we may also write
\begin{equation}
\label{eq:IS_data}
	y = H(x_0,T,\theta)+v,
\end{equation}
where $H$ is the function which is obtained by first running the model up to time $T$ followed by applying $h$ to the state $x_T$.  We are interested in the information we can extract from the data about the parameters $\theta$ and therefore consider the random variable $\theta|y$, which is characterized by its pdf $p(\theta|y)$. Using Bayes' rule, we find that
\begin{equation}
	p(\theta|y) \propto p(\theta)p(y|x_0,T,\theta),
\end{equation}
where  $p(\theta) = \mathcal{N}(\mu,\Sigma_0)$ is the ``prior'' and $p(y|x_0,T,\theta)$ is the ``likelihood'', which can be read off of the data equation (\ref{eq:IS_data}), $p(y|x_0,T,\theta)\sim \mathcal{N}(H(x_0,\theta),\Sigma)$.
The pdf $p(\theta|y)$  is called the ``posterior'' and we wish to approximate it with implicit sampling. 

The general procedure is as follows. Define a function by
\begin{equation}
	F(\theta)= -\log \left(p(\theta)p(y|x_0,T,\theta)\right).
\end{equation}
Note that $F$ is a function of the uncertain parameters and that the minimizer of $F$ is the mode of the posterior. Thus, the high probability region of the posterior is the neighborhood of the minimizer of $F$ and we can identify this region via numerical minimization of $F$. To find samples in this region we solve (repeatedly) the algebraic equations
\begin{equation}
\label{eq:IS_sampling_eq}
	F(\theta)-\phi = \frac{1}{2}\xi^T\xi,
\end{equation}
where $\phi = \min F$ and $\xi$ is a Gaussian reference variable with mean zero and unit variance, i.e. the equations have a random right hand side (RHS). Note that the RHS is small with a high probability ($\xi$ has mean zero), which implies that the left hand side is also small with a high probability and, therefore, the solution is close to the minimizer of $F$ which is the mode of the posterior. Thus, the solutions of these equations (for different realizations of $\xi$) are in the neighborhood of the mode of the posterior, i.e. almost all samples are compatible with the data. Generating samples with low probability with respect to the data (as in standard MC sampling) is avoided by the minimization step. 

While the general method of attack is clear, many research questions arise when applying this sampling technique to the target applications. A characteristic of all our target applications is that there is a hierarchy of experiments. How to use this characteristic for successful and efficient sampling is a major research question we will address. In the data assimilation context in which particle filters originated, the algorithms move through a temporal sequence of data and sequentially update the state/parameters as data becomes available. The update rule comes from repeated applications of Bayes' rule and ultimately leads to a recursive formulation of the posterior. Here we need to modify the methodology to transition from one experiment to the next as we move through the hierarchy of experiments, i.e. we need to find the update rule for moving up in the hierarchy. For example, we can construct priors at a higher level of the hierarchy from posteriors at lower levels. For implicit sampling this amounts to finding a representation of the posterior at a lower level that can serve as a prior in the optimization at the next stage. Moreover, the theory and numerics can be intertwined here because the models obtained (as posteriors) at lower levels of the hierarchy may help with speeding up the minimizations required at higher levels of the hierarchy (see below for more detail).  Another central issue is determining whether or not moving through the hierarchy of experiments from the simplest to the most complex introduces bias in the parameter estimates. If this bias is found to be significant, strategies must be developed to reduce that bias. 

Another central research question is how to use implicit sampling for multi-scale problems, specifically battery simulations as a target application. The situation is similar to how to use implicit sampling across a hierarchy of experiments (see above). Specifically, we will address how information can be propagated efficiently across different scales. Suppose we have two scales $x_1$ and $x_2$, and one data set $y_1$. We are interested in approximating the pdf $p(x^0, x^1|b^1)$ which, by Bayes' rule and assuming that the data $y_1$ depends only on the scale $x^1$, factorizes such that
\begin{equation}
	p(x^0, x^1|b^1) \propto p(b^1|x^1) p(x^1|x^0) p(x^0).
\end{equation}
This is the update rule for going from scale $0$ to $1$. As more scales and more data become available, extending this formalism gives
\begin{equation}
	p(x^0,x^1,x^2 | y^1,y^2) \propto p(y^2 | x^2) p(x^2|x^1) p(x^0,x^1 | y^1),
\end{equation}
the update rule that connects the scale $x^2$ with the data $y^2$ and the information we have about the scales $x^0$ and $x^1$ given the data $y^1$. What is required for successful updating between scales is an observation model that connects the data at each scale with the model at that scale as well as ``scale coupling'' models of the form $p(x^{n+1}|x^n)$. There are two main research topics here. For one, we need to investigate the validity of the assumptions that (\emph{i}) the scale coupling models are ``Markovian'', i.e. they couple only two subsequent scales; and (\emph{ii}) that the observation models at each scale only depends on the state at that scale. In addition, we need to investigate implementation issues, e.g. how will the coupling between scales affect the required optimizations and subsequent solves of~(\ref{eq:IS_sampling_eq}). \MarginPar{Peter: can you check that this makes sense?}

Further extensions of the implicit sampling formalism are required for the combustion applications, where the observation function $h$ itself can be uncertain. For example, laser diagnostic measurements depend on the system state but there are parameters describing the laser interaction (so-called quenching coefficients) that have uncertainties as well. We plan on extending the implicit formalism to estimate uncertainties in the measurement mechanisms. \MarginPar{Is this too much about ``feature assimilation''? I expect this to be hard.}
Moreover, at high levels of the hierarchy in turbulent combustion, we are interested in finding parameters that generate  features one observes in the data which are insensitive to details of the state trajectory. In fact, we want to avoid trying to estimate a trajectory that is a point-wise fit to the trajectory given by a measurement because it is a hopeless task. On a more abstract level, this corresponds to the problem of translating qualitative behavior observed in the data into quantitative information about parameters (by using sampling techniques) and, to the best of our knowledge, this fundamental problem has not been addressed before. We anticipate that tackling this problem will require a careful re-evaluation and perhaps re-definition of the observation function $h$ and we plan to take first steps towards a methodological and quantitative assimilation of data features. 

There are also many implementation issues that to need be addressed, especially with respect to efficient scaling of the sampling algorithm on massively parallel computers. The minimization is the computational bottleneck of implicit sampling and its efficient implementation is crucial for the success of the method. Moreover, the minimization algorithm will likely depend on where we are in the hierarchy of experiments. One of our specific research topics will be to address efficient minimization at each level of the hierarchy, especially in view of massively parallel computer architectures. For example, we can consider coupling adjoint codes for gradient computations of $F$ to BFGS-type algorithms using a parallel optimizer such as TAO from Argonne. At high levels of the hierarchy, adjoint codes may be out of reach or too costly and time consuming to construct. In these cases, derivative free optimization methods must be considered. Another possibility is to use simplified models for the minimization. For example, we can borrow ideas from multi-grid and run the minimizations on a coarse grid, while doing e.g. forecasting on the fine grid. Alternatively, we can use a surrogate method, where a small number of forward simulations are used to generate a simplified model of the simulation. This model is then used in the optimization and its further refinement goes hand in hand with its use in seeking the minimum. Many research questions surround the interplay between optimization and sampling, particularly with approximate surrogate models. For example, we can use numerical experiments to find a characterization of how errors introduced by a simplified model impact the overall behavior of the algorithm and what must be known about the errors of the simplified model to obtain such a characterization.   \MarginPar{George: The optimization algorithm is sequential.  It is going to be a bottleneck in a sampling algorithms where everything else can be embarrassingly parallelized on an exascale machine.  }

However, developing reduced order models for complex numerical models is not an easy task and the required computational overhead may not be justifiable.  One feasible approach is to reduce the complexity of the response that reduced order models are required to emulate by modeling only the difference between the simple and the complex models.  A simple Gaussian model similar to $v$ is unlikely to be accurate since we expect structural differences between the models.  A promising approach that is consistent with  (~\ref{eq:IS_data}) is Gaussian process regression.  Lets consider outputs from two different models, $H^S$ and $H^C$ where $H^C$ is deemed more accurate than $H^S$.  Then we can write (\ref{eq:IS_data}) as 
\begin{equation}
y(\theta) = H^S(\theta) + (H^C(\theta) - H^S(\theta)) + v.
\end{equation}
Gaussian process regression can then be used to model  $(H^C - H^S)$ as $\mathcal{N}(m_{\rm GPR}(\theta;\bar{\theta}),\Sigma_{\rm GPR}(\theta;\bar{\theta}))$, where $\bar{\theta}$ are sample points used to construct the Gaussian process regression model.  We note that both $m_{\rm GPR}$ and $\Sigma_{\rm GPR}$ depend on $\theta$ and as such is capable of modeling the nonlinearity in the difference.  The rest of the formulation used for implicit sampling can be then be used without modifications.  Appropriate models for $m_{\rm GPR}$ and $\Sigma_{\rm GPR}$ and the construction procedure the best utilizes exascale computer are both research questions that need to be answer. 

%\MarginPar{JG doubts this will work and notes a need to for derivative information to guide sampling. JBB
%doubts we can perform adjoint simulations for 3 turbulent simulations.  Only way out JB can think of
%is based on discussion with George about combining coarse simulation with a statistical surrogate to
%build an estimate of finer response . . . Unless someone has a brilliant idea here i suggest we leave this
%for the preproposal and thing of how to address it in real proposal}
\MarginPar{need to decide if there is something viable here. Another alternative that merits consideration
is using the adjoint of a simpler model for the optimization.}

Several other practical issues will be addressed. For example, we may have more than one data set at time $T$, i.e. we have data at $T_1,T_2,\dots,T_m$, where $m$ may be large. There are two options for using these data for estimation: (\emph{i}) we can extend the above formalism to include all $m$ data sets and estimate the parameters using all the data (off-line estimation); or (\emph{ii}) we can estimate the parameters using a batch of $k_1$ data sets and then refine this estimate using the remaining data in batches of $k_n$ sets, i.e. we can move through the data sequentially (on-line estimation). Theoretically, off-line estimation seems more attractive, since more data should lead to more accurate estimates because it avoids bias (e.g. the maximum likelihood estimator is asymptotically unbiased as the number of data goes to infinity). However in practice one often finds an ``optimal'' number of data sets per estimation sweep (i.e. an optimal $k$), the reason being (at least in part) that the exact model is not in the family being fit (even at high levels of  accuracy). This is well known and cleverly used in numerical weather prediction and we plan to investigate the optimal number of data sets per estimation sweep for our target applications.

\MarginPar{Matti: not sure this is necessary. Perhaps this is too specific/confusing?}
 Recall that we used Gaussian assumptions for the prior and likelihood in the presentation above, however these assumptions can be relaxed. For example, a Gaussian prior is not meaningful if the parameter is known to be positive. Similarly, we have chosen a Gaussian ``reference variable'' $\xi$, but other choices are also possible. We plan to investigate the interplay between priors, likelihood functions and the reference variable and will determine good choices for reference variables for each target application and at each level of the hierarchy. Similarly, there are various methods for solving the algebraic equations of implicit sampling (\ref{eq:IS_sampling_eq}). For example, if information about the curvature of $F$ is available, e.g. from BFGS-type optimizations, we can use linear maps based on the Hessian of $F$. In other situations, e.g. derivative free optimization or optimization using surrogates, this information may not be available. We plan to investigate optimal choices for solving the algebraic equations for each target application and at each level of the hierarchy.


\MarginPar{need to articulate research questions in more detail and discuss all the ones i'm not thinking about. Matti: I tried to address this above.}






 
\newpage
Although MCMC approaches are considerably more cost effective than naive sampling approaches, they can
exhibit a critical slowing down phenomenon for poorly conditioned problems.  The length of time that
one needs to simulate the Markov process to obtain good results depends on the autocorrelation time
of the underlying process. When the correlation time is long, the system must be run for extended periods
of time to obtain a good statistical characterization.
There are several approaches to dealing with this problem.
\MarginPar{Jonathan, can you describe them.  Need to set up a mathematical notation to talk about MCMC as well}

Another key problem that has to be addressed for effective sampling is dealing with model degeneracy.
A model has an approximate degeneracy if many different parameter sets are nearly as good at explaining the data.
For example, if there are multiple reaction pathways in a kinetic description,
certain combinations of reaction rates may be much better
estimated than the individual rates.
In such cases, isotropic sampling algorithms such as single variable heat bath (Gibbs sampler) or isotropic
Metropolis walk will be slow.
In the context of MCMC, we plan to use the affine sampling approaches developed at NYU to address issues of model degeneracy.
\MarginPar{what does this look like for particle filters?  guess is that optimization slows down.  answer regularized by prior? Matti: I attempt to make a connection to sampling here, but I might be totally wrong. Please check. We could also use this as the bridge to MCMC, because I  think that tackling degeneracy with IS is hard.}
Model degeneracy can lead to difficulties with implicit sampling, since it corresponds to a ``valley'' in  the function $F$, which slows down the convergence of the required optimization. Moreover, a good approximation of the degeneracy requires  the valley being populating with samples which will increase the number of samples one needs to generate. The computational expense of implicit sampling will thus increase significantly and we plan to switch to MCMC sampling to address sampling degeneracy. Moreover, we will investigate how to link MCMC to implicit sampling to speed up exploration of model degeneracies.


Finally, it is important to use statistical methods that are robust in the inevitable situation where the exact model is not in
the family being fit.
Models are only approximations to reality.
The Arrhenius form for reaction rates are only modeling approximations, though they can be
very accurate.
Similarly, typical models for species diffusion in combustion are only approximations, even at the continuum level,
to a full transport model, which is, in itself, an approximation to the underlying molecular processes.
Dven small modeling errors can make an accurate global fit impossible, particularly in chaotic systems.
A more serious structural issue arises in the case of batteries and photovoltaic models where the models
are extremely less mature.  In these areas we are likely to encounter structural models where key physical
processes are missing from the description.
One approach is to include noise in the dynamics, so that the posterior distribution does not require the
dynamical equations to be satisfied exactly.
We will conduct computational experiments to study this problem, then use the results to 
choose appropriate noise levels for our physical models.
By combining what is known about the error levels in both model and data, we can also estimate upper and lower bounds
of the predictive skills of the resulting stochastic models.
\MarginPar{can be develop criteria to assess if something is missing form model}

USING SIMPLIFIED MODELS SEEMS TO RAISE A SIMILAR SET OF ISSUES

MULTISCALE CHARACTER OF BATTERY PROBLEM

ROLE OF REDUCED ORDER MODELS

OTHER RESEARCH ISSUES:

One key element we need to investigate is how well our methodology deals with the
complex relationship between the state of the system and the data.  We can investigate
this issue by using synthetic experiments that transition from viewing the data
as a simple projection of the state through increasing levels of complexity up to
a computational model of the actual measurement.  From this type of experiment we can 
analyze how different aspects of the measurement alter the information we can
obtain from the measurement.  This type of analysis will also allows us to assess the
role of noise in a highly nonlinear observations of the state.

STUFF FROM WHITE PAER THAT I HAVEN'T EDITED YET

As noted above, reduced order models can be used to construct surrogates as part of a derivative free optimization strategy
to guide the sampling procedure. However, our goal is to also use reduced order models for an {\it {a posteriori}} UQ analysis.
For this purpose, the selection of an appropriate reduced order model at each level of the hierarchy depends on the
structure of the system at this level, ranging from model reduction approaches for models with smooth solution manifolds
to statistical approaches for complex models.
Given a hierarchy of reduced order models, this proposal will address how we can perform an integrated UQ analysis 
across the hierarchy, taking into account different levels of scale and fidelity.
%Given an appropriate reduced order model, after we have completed a suite of samples across the hierarchy
%we can then use the reduced order models to perform an uncertainty quantification analysis across
%the hierarchy.  
This analysis not only defines the predictive capability of the models, it also identifies
the major factors contributing to uncertainty and identifies what additional data would most significantly
impact the fidelity of predictions.

To evaluate the methodology we will examine test cases within each of our target application areas.  In particular,
we will consider thermodiffusive instabilities in turbulent hydrogen flames, robustness of Li-ion batteries under abusive conditions
and the formation of point defects in new photovoltaic materials.  In each case, we will focus on how uncertainties in the 
model parameters influence predictive capability and how we can reduce uncertainty using a hierarchy of experimental data.


\subsection*{Timetable of Activities}

\subsection*{Project Objectives}

The goal of this project is to develop a mathematical framework  
based on novel sampling methods that
intertwines parameter estimation and simulation 
to estimate uncertainty and improve prediction for target systems.
Specifically, we want to
(1) use available data from a hierarchy
of experiments of increasing scale and complexity to restrict
uncertainty in the description of the system, (2) estimate the impact of the improved characterization
on predictive capability and (3) identify which of the remaining uncertainties have the most impact
on the uncertainty of predictions.
We will demonstrate the use of the framework for prototype problems in combustion,
novel photovoltaic material and lithium-ion batteries.


\bibliographystyle{plain}

\bibliography{george_rom.bib} 

\end{document}
